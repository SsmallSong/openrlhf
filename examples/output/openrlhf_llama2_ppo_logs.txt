++ read -r -d '' training_commands
++ [[ '' != \s\l\u\r\m ]]
++ deepspeed ../train_ppo.py --pretrain /root/.cache/huggingface/hub/llama2_sft_mirror/ --reward_pretrain OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt --save_path /root/.cache/huggingface/hub/7b_llama_ppo_openrlhf --save_steps 500 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 2 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 512 --generate_max_len 1024 --zero_stage 2 --bf16 --actor_learning_rate 5e-7 --critic_learning_rate 9e-6 --init_kl_coef 0.01 --prompt_data trl-internal-testing/hh-rlhf-trl-style --prompt_data_probs 1.0 --max_samples 162000 --normalize_reward --actor_init_on_gpu --adam_offload --gradient_checkpointing
[2024-06-26 17:43:52,242] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:43:53,026] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-06-26 17:43:53,026] [INFO] [runner.py:568:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ../train_ppo.py --pretrain /root/.cache/huggingface/hub/llama2_sft_mirror/ --reward_pretrain OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt --save_path /root/.cache/huggingface/hub/7b_llama_ppo_openrlhf --save_steps 500 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 2 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 512 --generate_max_len 1024 --zero_stage 2 --bf16 --actor_learning_rate 5e-7 --critic_learning_rate 9e-6 --init_kl_coef 0.01 --prompt_data trl-internal-testing/hh-rlhf-trl-style --prompt_data_probs 1.0 --max_samples 162000 --normalize_reward --actor_init_on_gpu --adam_offload --gradient_checkpointing
[2024-06-26 17:43:55,053] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:43:55,631] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.19.stable.20231214+cuda12.3
[2024-06-26 17:43:55,631] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-06-26 17:43:55,631] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-06-26 17:43:55,631] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-06-26 17:43:55,632] [INFO] [launch.py:163:main] dist_world_size=4
[2024-06-26 17:43:55,632] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-06-26 17:43:55,632] [INFO] [launch.py:253:main] process 13852 spawned with command: ['/usr/bin/python', '-u', '../train_ppo.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/llama2_sft_mirror/', '--reward_pretrain', 'OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt', '--save_path', '/root/.cache/huggingface/hub/7b_llama_ppo_openrlhf', '--save_steps', '500', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '2', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '512', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--actor_learning_rate', '5e-7', '--critic_learning_rate', '9e-6', '--init_kl_coef', '0.01', '--prompt_data', 'trl-internal-testing/hh-rlhf-trl-style', '--prompt_data_probs', '1.0', '--max_samples', '162000', '--normalize_reward', '--actor_init_on_gpu', '--adam_offload', '--gradient_checkpointing']
[2024-06-26 17:43:55,633] [INFO] [launch.py:253:main] process 13853 spawned with command: ['/usr/bin/python', '-u', '../train_ppo.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/llama2_sft_mirror/', '--reward_pretrain', 'OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt', '--save_path', '/root/.cache/huggingface/hub/7b_llama_ppo_openrlhf', '--save_steps', '500', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '2', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '512', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--actor_learning_rate', '5e-7', '--critic_learning_rate', '9e-6', '--init_kl_coef', '0.01', '--prompt_data', 'trl-internal-testing/hh-rlhf-trl-style', '--prompt_data_probs', '1.0', '--max_samples', '162000', '--normalize_reward', '--actor_init_on_gpu', '--adam_offload', '--gradient_checkpointing']
[2024-06-26 17:43:55,634] [INFO] [launch.py:253:main] process 13854 spawned with command: ['/usr/bin/python', '-u', '../train_ppo.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/llama2_sft_mirror/', '--reward_pretrain', 'OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt', '--save_path', '/root/.cache/huggingface/hub/7b_llama_ppo_openrlhf', '--save_steps', '500', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '2', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '512', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--actor_learning_rate', '5e-7', '--critic_learning_rate', '9e-6', '--init_kl_coef', '0.01', '--prompt_data', 'trl-internal-testing/hh-rlhf-trl-style', '--prompt_data_probs', '1.0', '--max_samples', '162000', '--normalize_reward', '--actor_init_on_gpu', '--adam_offload', '--gradient_checkpointing']
[2024-06-26 17:43:55,635] [INFO] [launch.py:253:main] process 13855 spawned with command: ['/usr/bin/python', '-u', '../train_ppo.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/llama2_sft_mirror/', '--reward_pretrain', 'OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt', '--save_path', '/root/.cache/huggingface/hub/7b_llama_ppo_openrlhf', '--save_steps', '500', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '2', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '512', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--actor_learning_rate', '5e-7', '--critic_learning_rate', '9e-6', '--init_kl_coef', '0.01', '--prompt_data', 'trl-internal-testing/hh-rlhf-trl-style', '--prompt_data_probs', '1.0', '--max_samples', '162000', '--normalize_reward', '--actor_init_on_gpu', '--adam_offload', '--gradient_checkpointing']
[2024-06-26 17:43:59,300] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:43:59,369] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/.local/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-06-26 17:43:59,607] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:43:59,622] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/.local/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-06-26 17:43:59,922] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-26 17:43:59,922] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-06-26 17:44:00,008] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][2024-06-26 17:44:00,508] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-26 17:44:00,511] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.83s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.17s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.63s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.30s/it]
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.50s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.49s/it]
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.87s/it]/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.34s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.57s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.26s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.38s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.26s/it]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.67s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.24s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.05s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.19s/it]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:09,  4.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.38s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.61s/it]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.39s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.65s/it]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.13s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.93s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.00s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.28s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.24s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.41s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.89s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.09s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.32s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.19s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.41s/it]
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 4096, padding_idx=0)
      (layers): ModuleList(
        (0-31): 32 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
  )
)
CriticModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=2)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.15s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.40s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.54s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.40s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.55s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.05s/it]
reward normalization status: True
mean: tensor([0.3262], dtype=torch.bfloat16), std tensor([1.8516], dtype=torch.bfloat16)
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.813708543777466 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000000, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  4.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.58s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.29s/it]
Adam Optimizer #1 is created with AVX2 arithmetic capability.
Config: alpha=0.000009, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4135570526123047 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000000, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4354991912841797 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000000, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4270734786987305 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000000, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Adam Optimizer #1 is created with AVX2 arithmetic capability.
Config: alpha=0.000009, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
dataset: trl-internal-testing/hh-rlhf-trl-style
Adam Optimizer #1 is created with AVX2 arithmetic capability.
Config: alpha=0.000009, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Adam Optimizer #1 is created with AVX2 arithmetic capability.
Config: alpha=0.000009, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[Dataset({
    features: ['chosen', 'rejected', 'prompt'],
    num_rows: 160800
})]
  0%|          | 0/160800 [00:00<?, ?it/s]  1%|          | 1468/160800 [00:00<00:10, 14677.28it/s]  2%|▏         | 2979/160800 [00:00<00:10, 14931.46it/s]  3%|▎         | 4534/160800 [00:00<00:10, 15213.11it/s]  4%|▍         | 6056/160800 [00:00<00:10, 15051.76it/s]  5%|▍         | 7564/160800 [00:00<00:10, 15061.34it/s]  6%|▌         | 9071/160800 [00:00<00:10, 14944.49it/s]  7%|▋         | 10592/160800 [00:00<00:09, 15030.36it/s]  8%|▊         | 12130/160800 [00:00<00:09, 15139.37it/s]  9%|▊         | 13737/160800 [00:00<00:09, 15428.38it/s] 10%|▉         | 15311/160800 [00:01<00:09, 15523.68it/s] 11%|█         | 16914/160800 [00:01<00:09, 15675.74it/s] 11%|█▏        | 18482/160800 [00:01<00:09, 15654.62it/s] 12%|█▏        | 20048/160800 [00:01<00:09, 15580.34it/s] 13%|█▎        | 21607/160800 [00:01<00:08, 15553.28it/s] 14%|█▍        | 23163/160800 [00:01<00:08, 15363.12it/s] 15%|█▌        | 24700/160800 [00:01<00:08, 15316.96it/s] 16%|█▋        | 26236/160800 [00:01<00:08, 15328.17it/s] 17%|█▋        | 27780/160800 [00:01<00:08, 15361.15it/s] 18%|█▊        | 29338/160800 [00:01<00:08, 15426.27it/s] 19%|█▉        | 30904/160800 [00:02<00:08, 15495.73it/s] 20%|██        | 32457/160800 [00:02<00:08, 15503.58it/s] 21%|██        | 34008/160800 [00:02<00:08, 15475.96it/s] 22%|██▏       | 35556/160800 [00:02<00:08, 15456.42it/s] 23%|██▎       | 37111/160800 [00:02<00:07, 15483.98it/s] 24%|██▍       | 38660/160800 [00:02<00:08, 15206.05it/s] 25%|██▍       | 40196/160800 [00:02<00:07, 15248.24it/s] 26%|██▌       | 41723/160800 [00:02<00:07, 15253.60it/s] 27%|██▋       | 43278/160800 [00:02<00:07, 15341.29it/s] 28%|██▊       | 44813/160800 [00:02<00:07, 15287.74it/s] 29%|██▉       | 46343/160800 [00:03<00:07, 15199.55it/s] 30%|██▉       | 47890/160800 [00:03<00:07, 15278.42it/s] 31%|███       | 49430/160800 [00:03<00:07, 15313.22it/s] 32%|███▏      | 50971/160800 [00:03<00:07, 15341.80it/s] 33%|███▎      | 52506/160800 [00:03<00:07, 15343.50it/s] 34%|███▎      | 54041/160800 [00:03<00:07, 15189.88it/s] 35%|███▍      | 55610/160800 [00:03<00:06, 15337.98it/s] 36%|███▌      | 57149/160800 [00:03<00:06, 15352.51it/s] 36%|███▋      | 58685/160800 [00:03<00:06, 15257.50it/s] 37%|███▋      | 60212/160800 [00:03<00:06, 15109.39it/s] 38%|███▊      | 61724/160800 [00:04<00:06, 14843.30it/s] 39%|███▉      | 63301/160800 [00:04<00:06, 15112.68it/s] 40%|████      | 64856/160800 [00:04<00:06, 15240.01it/s] 41%|████▏     | 66385/160800 [00:04<00:06, 15252.59it/s] 42%|████▏     | 67912/160800 [00:04<00:06, 15245.41it/s] 43%|████▎     | 69438/160800 [00:04<00:06, 15019.76it/s] 44%|████▍     | 70976/160800 [00:04<00:05, 15125.46it/s] 45%|████▌     | 72535/160800 [00:04<00:05, 15262.04it/s] 46%|████▌     | 74090/160800 [00:04<00:05, 15344.67it/s] 47%|████▋     | 75627/160800 [00:04<00:05, 15351.84it/s] 48%|████▊     | 77163/160800 [00:05<00:05, 15196.78it/s] 49%|████▉     | 78684/160800 [00:05<00:05, 15153.44it/s] 50%|████▉     | 80200/160800 [00:05<00:05, 14768.49it/s] 51%|█████     | 81679/160800 [00:05<00:05, 14477.35it/s] 52%|█████▏    | 83129/160800 [00:05<00:05, 14257.34it/s] 53%|█████▎    | 84561/160800 [00:05<00:05, 14271.88it/s] 54%|█████▎    | 86096/160800 [00:05<00:05, 14585.31it/s] 54%|█████▍    | 87616/160800 [00:05<00:04, 14765.33it/s] 55%|█████▌    | 89184/160800 [00:05<00:04, 15036.14it/s] 56%|█████▋    | 90755/160800 [00:05<00:04, 15234.17it/s] 57%|█████▋    | 92368/160800 [00:06<00:04, 15500.03it/s] 58%|█████▊    | 93993/160800 [00:06<00:04, 15721.12it/s] 59%|█████▉    | 95566/160800 [00:06<00:04, 15671.96it/s] 60%|██████    | 97134/160800 [00:06<00:04, 15592.36it/s] 61%|██████▏   | 98694/160800 [00:06<00:03, 15562.85it/s] 62%|██████▏   | 100251/160800 [00:06<00:03, 15332.85it/s] 63%|██████▎   | 101821/160800 [00:06<00:03, 15439.59it/s] 64%|██████▍   | 103434/160800 [00:06<00:03, 15644.10it/s] 65%|██████▌   | 105013/160800 [00:06<00:03, 15686.22it/s] 66%|██████▋   | 106599/160800 [00:06<00:03, 15736.44it/s]Sample 1: What are some cuss words in english?
Sample 2: What kind of noises did dinosaurs make?
Sample 3: If you were going to steal from a convenience store, do you think it would be better in the morning or at night?
[rank2]: Traceback (most recent call last):
[rank2]:   File "/openrlhf/examples/scripts/../train_ppo.py", line 366, in <module>
[rank2]:     train(args)
[rank2]:   File "/openrlhf/examples/scripts/../train_ppo.py", line 120, in train
[rank2]:     kill
[rank2]: NameError: name 'kill' is not defined
 67%|██████▋   | 108174/160800 [00:07<00:03, 15734.19it/s] 68%|██████▊   | 109770/160800 [00:07<00:03, 15800.28it/s] 69%|██████▉   | 111360/160800 [00:07<00:03, 15829.64it/s] 70%|███████   | 112958/160800 [00:07<00:03, 15872.12it/s] 71%|███████   | 114546/160800 [00:07<00:02, 15741.38it/s] 72%|███████▏  | 116144/160800 [00:07<00:02, 15810.04it/s] 73%|███████▎  | 117726/160800 [00:07<00:02, 15727.47it/s] 74%|███████▍  | 119299/160800 [00:07<00:02, 15597.16it/s] 75%|███████▌  | 120878/160800 [00:07<00:02, 15651.07it/s] 76%|███████▌  | 122465/160800 [00:07<00:02, 15715.88it/s] 77%|███████▋  | 124037/160800 [00:08<00:02, 15635.73it/s] 78%|███████▊  | 125614/160800 [00:08<00:02, 15673.49it/s] 79%|███████▉  | 127187/160800 [00:08<00:02, 15688.84it/s] 80%|████████  | 128757/160800 [00:08<00:02, 15441.46it/s][2024-06-26 17:45:14,756] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 13852
[2024-06-26 17:45:15,334] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 13853
[2024-06-26 17:45:15,831] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 13854
[2024-06-26 17:45:15,831] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 13855
[2024-06-26 17:45:16,369] [ERROR] [launch.py:322:sigkill_handler] ['/usr/bin/python', '-u', '../train_ppo.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/llama2_sft_mirror/', '--reward_pretrain', 'OpenLLMAI/Llama-2-7b-rm-anthropic_hh-lmsys-oasst-webgpt', '--save_path', '/root/.cache/huggingface/hub/7b_llama_ppo_openrlhf', '--save_steps', '500', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '2', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '512', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--actor_learning_rate', '5e-7', '--critic_learning_rate', '9e-6', '--init_kl_coef', '0.01', '--prompt_data', 'trl-internal-testing/hh-rlhf-trl-style', '--prompt_data_probs', '1.0', '--max_samples', '162000', '--normalize_reward', '--actor_init_on_gpu', '--adam_offload', '--gradient_checkpointing'] exits with return code = 1
