++ read -r -d '' training_commands
++ [[ '' != \s\l\u\r\m ]]
++ deepspeed ../train_ppo.py --pretrain meta-llama/Meta-Llama-3-8B-Instruct --reward_pretrain OpenLLMAI/Llama-3-8b-rm-mixture --save_path /root/.cache/huggingface/hub/7b_llama3_inst_ppo_openrlhf --save_steps 20 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 2 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 512 --generate_max_len 512 --zero_stage 2 --bf16 --actor_learning_rate 5e-7 --critic_learning_rate 9e-6 --init_kl_coef 0.01 --prompt_data princeton-nlp/llama3-ultrafeedback --prompt_data_probs 1.0 --max_samples 162000 --normalize_reward --actor_init_on_gpu --adam_offload --gradient_checkpointing --apply_chat_template --input_key prompt
[2024-07-02 06:31:18,956] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 06:31:19,767] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-02 06:31:19,767] [INFO] [runner.py:568:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ../train_ppo.py --pretrain meta-llama/Meta-Llama-3-8B-Instruct --reward_pretrain OpenLLMAI/Llama-3-8b-rm-mixture --save_path /root/.cache/huggingface/hub/7b_llama3_inst_ppo_openrlhf --save_steps 20 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 2 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 512 --generate_max_len 512 --zero_stage 2 --bf16 --actor_learning_rate 5e-7 --critic_learning_rate 9e-6 --init_kl_coef 0.01 --prompt_data princeton-nlp/llama3-ultrafeedback --prompt_data_probs 1.0 --max_samples 162000 --normalize_reward --actor_init_on_gpu --adam_offload --gradient_checkpointing --apply_chat_template --input_key prompt
[2024-07-02 06:31:21,787] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 06:31:22,340] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.19.stable.20231214+cuda12.3
[2024-07-02 06:31:22,340] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-07-02 06:31:22,340] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-07-02 06:31:22,340] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-07-02 06:31:22,340] [INFO] [launch.py:163:main] dist_world_size=4
[2024-07-02 06:31:22,340] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-07-02 06:31:22,341] [INFO] [launch.py:253:main] process 56412 spawned with command: ['/usr/bin/python', '-u', '../train_ppo.py', '--local_rank=0', '--pretrain', 'meta-llama/Meta-Llama-3-8B-Instruct', '--reward_pretrain', 'OpenLLMAI/Llama-3-8b-rm-mixture', '--save_path', '/root/.cache/huggingface/hub/7b_llama3_inst_ppo_openrlhf', '--save_steps', '20', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '2', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '512', '--generate_max_len', '512', '--zero_stage', '2', '--bf16', '--actor_learning_rate', '5e-7', '--critic_learning_rate', '9e-6', '--init_kl_coef', '0.01', '--prompt_data', 'princeton-nlp/llama3-ultrafeedback', '--prompt_data_probs', '1.0', '--max_samples', '162000', '--normalize_reward', '--actor_init_on_gpu', '--adam_offload', '--gradient_checkpointing', '--apply_chat_template', '--input_key', 'prompt']
[2024-07-02 06:31:22,342] [INFO] [launch.py:253:main] process 56413 spawned with command: ['/usr/bin/python', '-u', '../train_ppo.py', '--local_rank=1', '--pretrain', 'meta-llama/Meta-Llama-3-8B-Instruct', '--reward_pretrain', 'OpenLLMAI/Llama-3-8b-rm-mixture', '--save_path', '/root/.cache/huggingface/hub/7b_llama3_inst_ppo_openrlhf', '--save_steps', '20', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '2', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '512', '--generate_max_len', '512', '--zero_stage', '2', '--bf16', '--actor_learning_rate', '5e-7', '--critic_learning_rate', '9e-6', '--init_kl_coef', '0.01', '--prompt_data', 'princeton-nlp/llama3-ultrafeedback', '--prompt_data_probs', '1.0', '--max_samples', '162000', '--normalize_reward', '--actor_init_on_gpu', '--adam_offload', '--gradient_checkpointing', '--apply_chat_template', '--input_key', 'prompt']
[2024-07-02 06:31:22,343] [INFO] [launch.py:253:main] process 56414 spawned with command: ['/usr/bin/python', '-u', '../train_ppo.py', '--local_rank=2', '--pretrain', 'meta-llama/Meta-Llama-3-8B-Instruct', '--reward_pretrain', 'OpenLLMAI/Llama-3-8b-rm-mixture', '--save_path', '/root/.cache/huggingface/hub/7b_llama3_inst_ppo_openrlhf', '--save_steps', '20', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '2', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '512', '--generate_max_len', '512', '--zero_stage', '2', '--bf16', '--actor_learning_rate', '5e-7', '--critic_learning_rate', '9e-6', '--init_kl_coef', '0.01', '--prompt_data', 'princeton-nlp/llama3-ultrafeedback', '--prompt_data_probs', '1.0', '--max_samples', '162000', '--normalize_reward', '--actor_init_on_gpu', '--adam_offload', '--gradient_checkpointing', '--apply_chat_template', '--input_key', 'prompt']
[2024-07-02 06:31:22,344] [INFO] [launch.py:253:main] process 56415 spawned with command: ['/usr/bin/python', '-u', '../train_ppo.py', '--local_rank=3', '--pretrain', 'meta-llama/Meta-Llama-3-8B-Instruct', '--reward_pretrain', 'OpenLLMAI/Llama-3-8b-rm-mixture', '--save_path', '/root/.cache/huggingface/hub/7b_llama3_inst_ppo_openrlhf', '--save_steps', '20', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '2', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '512', '--generate_max_len', '512', '--zero_stage', '2', '--bf16', '--actor_learning_rate', '5e-7', '--critic_learning_rate', '9e-6', '--init_kl_coef', '0.01', '--prompt_data', 'princeton-nlp/llama3-ultrafeedback', '--prompt_data_probs', '1.0', '--max_samples', '162000', '--normalize_reward', '--actor_init_on_gpu', '--adam_offload', '--gradient_checkpointing', '--apply_chat_template', '--input_key', 'prompt']
[2024-07-02 06:31:26,179] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 06:31:26,229] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 06:31:26,369] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-07-02 06:31:26,443] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-07-02 06:31:27,081] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-02 06:31:27,101] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-02 06:31:27,102] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-02 06:31:27,109] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-02 06:31:27,158] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.29s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.97s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:04,  4.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:04,  4.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.12s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:03,  3.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:03,  3.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.12s/it]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.43s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.15s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.96s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.27s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:08,  4.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:04,  4.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.11s/it]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.14s/it]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.39s/it]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.47s/it]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.66s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.51s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.47s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:08,  4.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.39s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.25s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.43s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.08s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.09s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.76s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.01s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 4096)
      (layers): ModuleList(
        (0-31): 32 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
  )
)
CriticModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.65s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.92s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:08,  4.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:04,  4.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.97s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.19s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:03,  4.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:03,  3.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.14s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.11s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
reward normalization status: True
mean: tensor([-0.9688], dtype=torch.bfloat16), std tensor([3.5156], dtype=torch.bfloat16)
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.413078784942627 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000000, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4292802810668945 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000000, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Adam Optimizer #1 is created with AVX2 arithmetic capability.
Config: alpha=0.000009, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4276316165924072 seconds
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.448042631149292 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000000, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000000, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Adam Optimizer #1 is created with AVX2 arithmetic capability.
Config: alpha=0.000009, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Adam Optimizer #1 is created with AVX2 arithmetic capability.
Config: alpha=0.000009, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
Adam Optimizer #1 is created with AVX2 arithmetic capability.
Config: alpha=0.000009, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
dataset: princeton-nlp/llama3-ultrafeedback
[Dataset({
    features: ['prompt_id', 'prompt', 'all_generated_responses', 'all_rm_scores', 'chosen', 'rejected'],
    num_rows: 59876
})]
  0%|          | 0/59876 [00:00<?, ?it/s]  1%|          | 629/59876 [00:00<00:09, 6285.96it/s]  2%|▏         | 1437/59876 [00:00<00:07, 7338.37it/s]  4%|▍         | 2253/59876 [00:00<00:07, 7711.61it/s]  5%|▌         | 3061/59876 [00:00<00:07, 7854.73it/s]  6%|▋         | 3851/59876 [00:00<00:07, 7870.22it/s]  8%|▊         | 4639/59876 [00:00<00:07, 7844.29it/s]  9%|▉         | 5450/59876 [00:00<00:06, 7928.31it/s] 10%|█         | 6253/59876 [00:00<00:06, 7959.34it/s] 12%|█▏        | 7050/59876 [00:00<00:06, 7961.61it/s] 13%|█▎        | 7849/59876 [00:01<00:06, 7968.87it/s] 14%|█▍        | 8652/59876 [00:01<00:06, 7984.24it/s] 16%|█▌        | 9453/59876 [00:01<00:06, 7991.01it/s] 17%|█▋        | 10256/59876 [00:01<00:06, 7999.99it/s] 18%|█▊        | 11057/59876 [00:01<00:06, 7998.37it/s] 20%|█▉        | 11857/59876 [00:01<00:06, 7947.30it/s] 21%|██        | 12652/59876 [00:01<00:06, 7860.02it/s] 22%|██▏       | 13439/59876 [00:01<00:06, 7701.58it/s] 24%|██▍       | 14232/59876 [00:01<00:05, 7768.03it/s] 25%|██▌       | 15032/59876 [00:01<00:05, 7835.84it/s] 26%|██▋       | 15843/59876 [00:02<00:05, 7914.84it/s] 28%|██▊       | 16635/59876 [00:02<00:05, 7910.29it/s] 29%|██▉       | 17442/59876 [00:02<00:05, 7954.82it/s] 30%|███       | 18254/59876 [00:02<00:05, 8001.49it/s] 32%|███▏      | 19065/59876 [00:02<00:05, 8032.28it/s] 33%|███▎      | 19869/59876 [00:02<00:05, 7999.59it/s] 35%|███▍      | 20670/59876 [00:02<00:04, 7995.99it/s] 36%|███▌      | 21494/59876 [00:02<00:04, 8066.66it/s] 37%|███▋      | 22301/59876 [00:02<00:04, 8065.82it/s] 39%|███▊      | 23120/59876 [00:02<00:04, 8102.55it/s] 40%|███▉      | 23931/59876 [00:03<00:04, 8094.00it/s] 41%|████▏     | 24757/59876 [00:03<00:04, 8141.32it/s] 43%|████▎     | 25572/59876 [00:03<00:04, 8137.61it/s] 44%|████▍     | 26386/59876 [00:03<00:04, 8091.57it/s] 45%|████▌     | 27196/59876 [00:03<00:04, 8090.89it/s] 47%|████▋     | 28006/59876 [00:03<00:04, 7954.53it/s] 48%|████▊     | 28803/59876 [00:03<00:03, 7957.40it/s] 49%|████▉     | 29633/59876 [00:03<00:03, 8056.70it/s] 51%|█████     | 30443/59876 [00:03<00:03, 8066.84it/s] 52%|█████▏    | 31253/59876 [00:03<00:03, 8076.38it/s] 54%|█████▎    | 32075/59876 [00:04<00:03, 8116.74it/s] 55%|█████▍    | 32887/59876 [00:04<00:03, 8116.92it/s] 56%|█████▋    | 33699/59876 [00:04<00:03, 8108.09it/s] 58%|█████▊    | 34510/59876 [00:04<00:03, 8094.29it/s] 59%|█████▉    | 35320/59876 [00:04<00:03, 8077.75it/s] 60%|██████    | 36128/59876 [00:04<00:02, 7951.68it/s] 62%|██████▏   | 36924/59876 [00:04<00:02, 7900.01it/s] 63%|██████▎   | 37736/59876 [00:04<00:02, 7963.82it/s] 64%|██████▍   | 38533/59876 [00:04<00:02, 7833.46it/s] 66%|██████▌   | 39331/59876 [00:04<00:02, 7875.80it/s] 67%|██████▋   | 40156/59876 [00:05<00:02, 7984.84it/s] 68%|██████▊   | 40965/59876 [00:05<00:02, 8014.47it/s] 70%|██████▉   | 41767/59876 [00:05<00:02, 8006.38it/s]Sample 1: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Ques:One of the following sentences is nonsensical. Which one is it?
Options:
- Sentence A: "advertising is used to show the shortcomings of products"
- Sentence B: "advertising is used to show the advantages of products"

Ans:Let's see... Advertising is used to market products to increase sales.
Sentence A

Ques:Which of these sentences doesn't make sense?
Options:
- Sentence A: "fred turned up the volume of the radio"
- Sentence B: "fred turned up the capacity of the radio"

Ans:Let's see... Capacity is generally used to describe physical quantity, not volume.
Sentence B

Ques:Which of the following sentences is nonsensical?
Options:
- Sentence A: "he went to the kitchen because he felt sleepy"
- Sentence B: "he went to the bedroom because he felt sleepy"

Ans:Let's see... Going to the kitchen won't happen because feeling sleepy.
Sentence A

Ques:One of the following sentences is nonsensical. Which one is it?
Options:
- Sentence A: "She went to the supermarket, because she was out of food."
- Sentence B: "She went to the bathroom, because she was out of food."

Ans:<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 2: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Given the task definition and input, reply with output. Given a pair of words, deduce the type of relationship between them. The various types of relations are: 'HYPER', 'COORD' and 'RANDOM'. Let's denote the first word by X and the second word by Y. A COORD relationship holds when X and Y belong to the same semantic class. A HYPER relationship applies when X is a specific instance of Y. If niether COORD nor HYPER are applicable then the relationship is RANDOM.

X: know, Y: artefact<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 3: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

How can I use Python to create a personalized time management assistant that tracks my daily routine, analyzes my productivity levels, suggests areas of improvement, and adapts to my changing needs? Can you provide a code snippet that includes these features and integrates with my calendar and task management tools?<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 4: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Given the task definition, example input & output, solve the new input case.
In this task, you are given a tuple, comprising Head and Tail, separated with <sep>. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether, as a result of the Head, PersonY or others will want what is mentioned in the Tail or not. In this task, wanting is a postcondition desire on the part of PersonY and others, respectively. For example, as a result of PersonX giving PersonY gifts, PersonY may want to open the gift. Classify your answers into "Yes" and "No". The phrase may also contain "___", a placeholder that can be an object, a person, and/or an action.
Example: Head: PersonX holds PersonY's head<sep>Tail: to be messaged
Output: Yes
This is a good example. As a result of the Head, PersonY will want to be messaged.

New input case for you: Head: PersonX expresses PersonX's views<sep>Tail: to write on a notebook
Output:<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 5: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Describe the differences between the mechanisms of action in cognitive-behavioral and psychodynamic psychotherapies<|eot_id|><|start_header_id|>assistant<|end_header_id|>


467
 71%|███████   | 42579/59876 [00:05<00:02, 8038.16it/s] 72%|███████▏  | 43384/59876 [00:05<00:02, 8033.89it/s] 74%|███████▍  | 44188/59876 [00:05<00:01, 7957.33it/s] 75%|███████▌  | 44984/59876 [00:05<00:01, 7947.67it/s] 76%|███████▋  | 45780/59876 [00:05<00:01, 7950.17it/s] 78%|███████▊  | 46579/59876 [00:05<00:01, 7958.92it/s] 79%|███████▉  | 47395/59876 [00:05<00:01, 8018.20it/s] 81%|████████  | 48205/59876 [00:06<00:01, 8040.41it/s] 82%|████████▏ | 49010/59876 [00:06<00:01, 8025.15it/s] 83%|████████▎ | 49813/59876 [00:06<00:01, 8022.27it/s] 85%|████████▍ | 50617/59876 [00:06<00:01, 8026.77it/s] 86%|████████▌ | 51428/59876 [00:06<00:01, 8048.98it/s] 87%|████████▋ | 52233/59876 [00:06<00:00, 7965.30it/s]Sample 1: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Ques:One of the following sentences is nonsensical. Which one is it?
Options:
- Sentence A: "advertising is used to show the shortcomings of products"
- Sentence B: "advertising is used to show the advantages of products"

Ans:Let's see... Advertising is used to market products to increase sales.
Sentence A

Ques:Which of these sentences doesn't make sense?
Options:
- Sentence A: "fred turned up the volume of the radio"
- Sentence B: "fred turned up the capacity of the radio"

Ans:Let's see... Capacity is generally used to describe physical quantity, not volume.
Sentence B

Ques:Which of the following sentences is nonsensical?
Options:
- Sentence A: "he went to the kitchen because he felt sleepy"
- Sentence B: "he went to the bedroom because he felt sleepy"

Ans:Let's see... Going to the kitchen won't happen because feeling sleepy.
Sentence A

Ques:One of the following sentences is nonsensical. Which one is it?
Options:
- Sentence A: "She went to the supermarket, because she was out of food."
- Sentence B: "She went to the bathroom, because she was out of food."

Ans:<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 2: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Given the task definition and input, reply with output. Given a pair of words, deduce the type of relationship between them. The various types of relations are: 'HYPER', 'COORD' and 'RANDOM'. Let's denote the first word by X and the second word by Y. A COORD relationship holds when X and Y belong to the same semantic class. A HYPER relationship applies when X is a specific instance of Y. If niether COORD nor HYPER are applicable then the relationship is RANDOM.

X: know, Y: artefact<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 3: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

How can I use Python to create a personalized time management assistant that tracks my daily routine, analyzes my productivity levels, suggests areas of improvement, and adapts to my changing needs? Can you provide a code snippet that includes these features and integrates with my calendar and task management tools?<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 4: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Given the task definition, example input & output, solve the new input case.
In this task, you are given a tuple, comprising Head and Tail, separated with <sep>. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether, as a result of the Head, PersonY or others will want what is mentioned in the Tail or not. In this task, wanting is a postcondition desire on the part of PersonY and others, respectively. For example, as a result of PersonX giving PersonY gifts, PersonY may want to open the gift. Classify your answers into "Yes" and "No". The phrase may also contain "___", a placeholder that can be an object, a person, and/or an action.
Example: Head: PersonX holds PersonY's head<sep>Tail: to be messaged
Output: Yes
This is a good example. As a result of the Head, PersonY will want to be messaged.

New input case for you: Head: PersonX expresses PersonX's views<sep>Tail: to write on a notebook
Output:<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 5: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Describe the differences between the mechanisms of action in cognitive-behavioral and psychodynamic psychotherapies<|eot_id|><|start_header_id|>assistant<|end_header_id|>


467
 89%|████████▊ | 53053/59876 [00:06<00:00, 8033.67it/s] 90%|████████▉ | 53866/59876 [00:06<00:00, 8062.13it/s]Sample 1: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Ques:One of the following sentences is nonsensical. Which one is it?
Options:
- Sentence A: "advertising is used to show the shortcomings of products"
- Sentence B: "advertising is used to show the advantages of products"

Ans:Let's see... Advertising is used to market products to increase sales.
Sentence A

Ques:Which of these sentences doesn't make sense?
Options:
- Sentence A: "fred turned up the volume of the radio"
- Sentence B: "fred turned up the capacity of the radio"

Ans:Let's see... Capacity is generally used to describe physical quantity, not volume.
Sentence B

Ques:Which of the following sentences is nonsensical?
Options:
- Sentence A: "he went to the kitchen because he felt sleepy"
- Sentence B: "he went to the bedroom because he felt sleepy"

Ans:Let's see... Going to the kitchen won't happen because feeling sleepy.
Sentence A

Ques:One of the following sentences is nonsensical. Which one is it?
Options:
- Sentence A: "She went to the supermarket, because she was out of food."
- Sentence B: "She went to the bathroom, because she was out of food."

Ans:<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 2: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Given the task definition and input, reply with output. Given a pair of words, deduce the type of relationship between them. The various types of relations are: 'HYPER', 'COORD' and 'RANDOM'. Let's denote the first word by X and the second word by Y. A COORD relationship holds when X and Y belong to the same semantic class. A HYPER relationship applies when X is a specific instance of Y. If niether COORD nor HYPER are applicable then the relationship is RANDOM.

X: know, Y: artefact<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 3: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

How can I use Python to create a personalized time management assistant that tracks my daily routine, analyzes my productivity levels, suggests areas of improvement, and adapts to my changing needs? Can you provide a code snippet that includes these features and integrates with my calendar and task management tools?<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 4: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Given the task definition, example input & output, solve the new input case.
In this task, you are given a tuple, comprising Head and Tail, separated with <sep>. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether, as a result of the Head, PersonY or others will want what is mentioned in the Tail or not. In this task, wanting is a postcondition desire on the part of PersonY and others, respectively. For example, as a result of PersonX giving PersonY gifts, PersonY may want to open the gift. Classify your answers into "Yes" and "No". The phrase may also contain "___", a placeholder that can be an object, a person, and/or an action.
Example: Head: PersonX holds PersonY's head<sep>Tail: to be messaged
Output: Yes
This is a good example. As a result of the Head, PersonY will want to be messaged.

New input case for you: Head: PersonX expresses PersonX's views<sep>Tail: to write on a notebook
Output:<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 5: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Describe the differences between the mechanisms of action in cognitive-behavioral and psychodynamic psychotherapies<|eot_id|><|start_header_id|>assistant<|end_header_id|>


467
 91%|█████████▏| 54673/59876 [00:06<00:00, 8004.22it/s] 93%|█████████▎| 55497/59876 [00:06<00:00, 8072.92it/s] 94%|█████████▍| 56323/59876 [00:07<00:00, 8127.82it/s] 95%|█████████▌| 57141/59876 [00:07<00:00, 8141.55it/s] 97%|█████████▋| 57956/59876 [00:07<00:00, 8142.72it/s] 98%|█████████▊| 58771/59876 [00:07<00:00, 8142.95it/s]100%|█████████▉| 59586/59876 [00:07<00:00, 8022.32it/s]100%|██████████| 59876/59876 [00:07<00:00, 7985.32it/s]
Sample 1: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Ques:One of the following sentences is nonsensical. Which one is it?
Options:
- Sentence A: "advertising is used to show the shortcomings of products"
- Sentence B: "advertising is used to show the advantages of products"

Ans:Let's see... Advertising is used to market products to increase sales.
Sentence A

Ques:Which of these sentences doesn't make sense?
Options:
- Sentence A: "fred turned up the volume of the radio"
- Sentence B: "fred turned up the capacity of the radio"

Ans:Let's see... Capacity is generally used to describe physical quantity, not volume.
Sentence B

Ques:Which of the following sentences is nonsensical?
Options:
- Sentence A: "he went to the kitchen because he felt sleepy"
- Sentence B: "he went to the bedroom because he felt sleepy"

Ans:Let's see... Going to the kitchen won't happen because feeling sleepy.
Sentence A

Ques:One of the following sentences is nonsensical. Which one is it?
Options:
- Sentence A: "She went to the supermarket, because she was out of food."
- Sentence B: "She went to the bathroom, because she was out of food."

Ans:<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 2: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Given the task definition and input, reply with output. Given a pair of words, deduce the type of relationship between them. The various types of relations are: 'HYPER', 'COORD' and 'RANDOM'. Let's denote the first word by X and the second word by Y. A COORD relationship holds when X and Y belong to the same semantic class. A HYPER relationship applies when X is a specific instance of Y. If niether COORD nor HYPER are applicable then the relationship is RANDOM.

X: know, Y: artefact<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 3: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

How can I use Python to create a personalized time management assistant that tracks my daily routine, analyzes my productivity levels, suggests areas of improvement, and adapts to my changing needs? Can you provide a code snippet that includes these features and integrates with my calendar and task management tools?<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 4: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Given the task definition, example input & output, solve the new input case.
In this task, you are given a tuple, comprising Head and Tail, separated with <sep>. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether, as a result of the Head, PersonY or others will want what is mentioned in the Tail or not. In this task, wanting is a postcondition desire on the part of PersonY and others, respectively. For example, as a result of PersonX giving PersonY gifts, PersonY may want to open the gift. Classify your answers into "Yes" and "No". The phrase may also contain "___", a placeholder that can be an object, a person, and/or an action.
Example: Head: PersonX holds PersonY's head<sep>Tail: to be messaged
Output: Yes
This is a good example. As a result of the Head, PersonY will want to be messaged.

New input case for you: Head: PersonX expresses PersonX's views<sep>Tail: to write on a notebook
Output:<|eot_id|><|start_header_id|>assistant<|end_header_id|>


Sample 5: <|begin_of_text|><|start_header_id|>user<|end_header_id|>

Describe the differences between the mechanisms of action in cognitive-behavioral and psychodynamic psychotherapies<|eot_id|><|start_header_id|>assistant<|end_header_id|>


467
[2024-07-02 06:32:40,759] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.5, git-hash=unknown, git-branch=unknown
[2024-07-02 06:32:40,759] [INFO] [comm.py:662:init_distributed] Distributed backend already initialized
[2024-07-02 06:32:42,153] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-02 06:32:42,154] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-07-02 06:32:42,154] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-07-02 06:32:42,162] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-07-02 06:32:42,162] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-07-02 06:32:42,162] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-07-02 06:32:42,162] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500,000,000
[2024-07-02 06:32:42,162] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500,000,000
[2024-07-02 06:32:42,162] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-07-02 06:32:42,162] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-07-02 06:33:00,761] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-07-02 06:33:00,762] [INFO] [utils.py:801:see_memory_usage] MA 15.94 GB         Max_MA 15.94 GB         CA 15.94 GB         Max_CA 16 GB 
[2024-07-02 06:33:00,762] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 239.87 GB, percent = 27.7%
[2024-07-02 06:33:03,698] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-07-02 06:33:03,699] [INFO] [utils.py:801:see_memory_usage] MA 15.94 GB         Max_MA 15.94 GB         CA 15.94 GB         Max_CA 16 GB 
[2024-07-02 06:33:03,699] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 287.51 GB, percent = 33.2%
[2024-07-02 06:33:03,699] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-07-02 06:33:04,049] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-07-02 06:33:04,050] [INFO] [utils.py:801:see_memory_usage] MA 15.94 GB         Max_MA 15.94 GB         CA 15.94 GB         Max_CA 16 GB 
[2024-07-02 06:33:04,050] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 281.2 GB, percent = 32.5%
[2024-07-02 06:33:04,052] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-07-02 06:33:04,053] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-07-02 06:33:04,053] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f9cc05e2cb0>
[2024-07-02 06:33:04,053] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-07-02 06:33:04,053] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9cc05e3ac0>
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-02 06:33:04,054] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   grad_accum_dtype ............. fp32
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 16
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-07-02 06:33:04,055] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   steps_per_print .............. 100
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  2
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-02 06:33:04,056] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-07-02 06:33:04,056] [INFO] [config.py:986:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 2, 
        "offload_param": {
            "device": "none"
        }, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "sub_group_size": "auto", 
        "stage3_max_live_parameters": "auto", 
        "stage3_max_reuse_distance": "auto", 
        "stage3_param_persistence_threshold": "auto", 
        "stage3_prefetch_bucket_size": "auto", 
        "reduce_bucket_size": "auto", 
        "zero_hpz_partition_size": 1, 
        "zero_quantized_weights": false, 
        "zero_quantized_gradients": false
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "data_types": {
        "grad_accum_dtype": "fp32"
    }, 
    "train_micro_batch_size_per_gpu": 2, 
    "train_batch_size": 128
}
[2024-07-02 06:33:04,057] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.5, git-hash=unknown, git-branch=unknown
[2024-07-02 06:33:04,057] [INFO] [comm.py:662:init_distributed] Distributed backend already initialized
[2024-07-02 06:33:09,006] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-02 06:33:09,008] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-07-02 06:33:09,008] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-07-02 06:33:09,015] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-07-02 06:33:09,015] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-07-02 06:33:09,015] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-07-02 06:33:09,016] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500,000,000
[2024-07-02 06:33:09,016] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500,000,000
[2024-07-02 06:33:09,016] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-07-02 06:33:09,016] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-07-02 06:33:25,663] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-07-02 06:33:25,664] [INFO] [utils.py:801:see_memory_usage] MA 30.89 GB         Max_MA 30.89 GB         CA 30.9 GB         Max_CA 31 GB 
[2024-07-02 06:33:25,664] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 268.79 GB, percent = 31.0%
[2024-07-02 06:33:28,600] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-07-02 06:33:28,601] [INFO] [utils.py:801:see_memory_usage] MA 30.89 GB         Max_MA 30.89 GB         CA 30.9 GB         Max_CA 31 GB 
[2024-07-02 06:33:28,601] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 267.94 GB, percent = 30.9%
[2024-07-02 06:33:28,601] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-07-02 06:33:28,762] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-07-02 06:33:28,763] [INFO] [utils.py:801:see_memory_usage] MA 30.89 GB         Max_MA 30.89 GB         CA 30.9 GB         Max_CA 31 GB 
[2024-07-02 06:33:28,763] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 269.59 GB, percent = 31.1%
[2024-07-02 06:33:28,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-07-02 06:33:28,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-07-02 06:33:28,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f9cc05e37f0>
[2024-07-02 06:33:28,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-07-02 06:33:28,767] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9cb4312170>
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-07-02 06:33:28,767] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   grad_accum_dtype ............. fp32
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 16
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-07-02 06:33:28,768] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   steps_per_print .............. 100
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  2
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-02 06:33:28,769] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-07-02 06:33:28,770] [INFO] [config.py:986:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 2, 
        "offload_param": {
            "device": "none"
        }, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "sub_group_size": "auto", 
        "stage3_max_live_parameters": "auto", 
        "stage3_max_reuse_distance": "auto", 
        "stage3_param_persistence_threshold": "auto", 
        "stage3_prefetch_bucket_size": "auto", 
        "reduce_bucket_size": "auto", 
        "zero_hpz_partition_size": 1, 
        "zero_quantized_weights": false, 
        "zero_quantized_gradients": false
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "data_types": {
        "grad_accum_dtype": "fp32"
    }, 
    "train_micro_batch_size_per_gpu": 2, 
    "train_batch_size": 128
}
[2024-07-02 06:33:28,770] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.5, git-hash=unknown, git-branch=unknown
[2024-07-02 06:33:28,770] [INFO] [comm.py:662:init_distributed] Distributed backend already initialized
[2024-07-02 06:33:36,494] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-02 06:33:36,495] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-07-02 06:33:36,721] [INFO] [utils.py:800:see_memory_usage] begin bf16_optimizer
[2024-07-02 06:33:36,722] [INFO] [utils.py:801:see_memory_usage] MA 44.87 GB         Max_MA 44.87 GB         CA 45.0 GB         Max_CA 45 GB 
[2024-07-02 06:33:36,722] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 225.03 GB, percent = 26.0%
[2024-07-02 06:33:36,884] [INFO] [utils.py:800:see_memory_usage] end bf16_optimizer
[2024-07-02 06:33:36,885] [INFO] [utils.py:801:see_memory_usage] MA 44.87 GB         Max_MA 44.87 GB         CA 45.0 GB         Max_CA 45 GB 
[2024-07-02 06:33:36,885] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 222.06 GB, percent = 25.6%
[2024-07-02 06:33:36,886] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9cc2b17ee0>
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-07-02 06:33:36,887] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 16
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-02 06:33:36,888] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   steps_per_print .............. 100
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  2
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-02 06:33:36,889] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2024-07-02 06:33:36,889] [INFO] [config.py:986:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 2, 
    "train_batch_size": 128
}
[2024-07-02 06:33:36,889] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.5, git-hash=unknown, git-branch=unknown
[2024-07-02 06:33:36,890] [INFO] [comm.py:662:init_distributed] Distributed backend already initialized
[2024-07-02 06:33:40,287] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-02 06:33:40,288] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
[2024-07-02 06:33:40,508] [INFO] [utils.py:800:see_memory_usage] begin bf16_optimizer
[2024-07-02 06:33:40,509] [INFO] [utils.py:801:see_memory_usage] MA 59.83 GB         Max_MA 59.83 GB         CA 60.08 GB         Max_CA 60 GB 
[2024-07-02 06:33:40,509] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 171.46 GB, percent = 19.8%
[2024-07-02 06:33:40,668] [INFO] [utils.py:800:see_memory_usage] end bf16_optimizer
[2024-07-02 06:33:40,669] [INFO] [utils.py:801:see_memory_usage] MA 59.83 GB         Max_MA 59.83 GB         CA 60.08 GB         Max_CA 60 GB 
[2024-07-02 06:33:40,669] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 171.45 GB, percent = 19.8%
[2024-07-02 06:33:40,670] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-07-02 06:33:40,670] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-02 06:33:40,670] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-02 06:33:40,670] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-07-02 06:33:40,670] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-07-02 06:33:40,670] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-02 06:33:40,670] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-07-02 06:33:40,670] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-07-02 06:33:40,670] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-07-02 06:33:40,670] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9cc2b20460>
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 16
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-07-02 06:33:40,671] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   steps_per_print .............. 100
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  2
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-07-02 06:33:40,672] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-07-02 06:33:40,673] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-02 06:33:40,673] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2024-07-02 06:33:40,673] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-02 06:33:40,673] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2024-07-02 06:33:40,673] [INFO] [config.py:986:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 2, 
    "train_batch_size": 128
}
Episode [1/1]:   0%|          | 0/3742 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
Episode [1/1]:   0%|          | 1/3742 [00:39<41:20:30, 39.78s/it]Episode [1/1]:   0%|          | 2/3742 [01:12<37:15:06, 35.86s/it]Episode [1/1]:   0%|          | 3/3742 [01:51<38:29:26, 37.06s/it]Episode [1/1]:   0%|          | 4/3742 [02:29<38:42:57, 37.29s/it]Episode [1/1]:   0%|          | 5/3742 [02:41<29:35:49, 28.51s/it]Episode [1/1]:   0%|          | 6/3742 [03:19<32:46:35, 31.58s/it]Episode [1/1]:   0%|          | 7/3742 [03:57<34:55:10, 33.66s/it]Episode [1/1]:   0%|          | 8/3742 [04:34<36:05:29, 34.80s/it]Episode [1/1]:   0%|          | 9/3742 [05:12<36:54:09, 35.59s/it]Episode [1/1]:   0%|          | 10/3742 [05:50<37:40:58, 36.35s/it]Episode [1/1]:   0%|          | 11/3742 [06:26<37:42:54, 36.39s/it]Episode [1/1]:   0%|          | 12/3742 [07:01<37:14:25, 35.94s/it]Episode [1/1]:   0%|          | 13/3742 [07:38<37:33:35, 36.26s/it]Episode [1/1]:   0%|          | 14/3742 [08:16<37:58:31, 36.67s/it]Episode [1/1]:   0%|          | 15/3742 [08:43<35:10:00, 33.97s/it]Episode [1/1]:   0%|          | 16/3742 [09:20<36:10:07, 34.95s/it]Episode [1/1]:   0%|          | 17/3742 [09:59<37:11:25, 35.94s/it]Episode [1/1]:   0%|          | 18/3742 [10:36<37:36:13, 36.35s/it]Episode [1/1]:   1%|          | 19/3742 [11:14<38:09:52, 36.90s/it]Episode [1/1]:   1%|          | 20/3742 [11:52<38:30:20, 37.24s/it]Episode [1/1]:   1%|          | 21/3742 [12:30<38:33:30, 37.30s/it]Episode [1/1]:   1%|          | 22/3742 [13:09<39:11:34, 37.93s/it]Episode [1/1]:   1%|          | 23/3742 [13:26<32:36:04, 31.56s/it]Episode [1/1]:   1%|          | 24/3742 [14:03<34:21:11, 33.26s/it]Episode [1/1]:   1%|          | 25/3742 [14:40<35:32:36, 34.42s/it]Episode [1/1]:   1%|          | 26/3742 [15:19<37:00:17, 35.85s/it]Episode [1/1]:   1%|          | 27/3742 [15:58<37:48:05, 36.63s/it]Episode [1/1]:   1%|          | 28/3742 [16:30<36:23:30, 35.27s/it]Episode [1/1]:   1%|          | 29/3742 [16:49<31:12:50, 30.26s/it]Episode [1/1]:   1%|          | 30/3742 [17:27<33:36:08, 32.59s/it]Episode [1/1]:   1%|          | 31/3742 [18:06<35:43:57, 34.66s/it]Episode [1/1]:   1%|          | 32/3742 [18:43<36:22:59, 35.30s/it]Episode [1/1]:   1%|          | 33/3742 [19:20<36:56:10, 35.85s/it]Episode [1/1]:   1%|          | 34/3742 [19:59<38:03:32, 36.95s/it]Episode [1/1]:   1%|          | 35/3742 [20:38<38:33:32, 37.45s/it]Episode [1/1]:   1%|          | 36/3742 [21:12<37:24:45, 36.34s/it]Episode [1/1]:   1%|          | 37/3742 [21:49<37:40:19, 36.60s/it]Episode [1/1]:   1%|          | 38/3742 [22:27<37:56:52, 36.88s/it]Episode [1/1]:   1%|          | 39/3742 [23:06<38:47:13, 37.71s/it]Episode [1/1]:   1%|          | 40/3742 [23:21<31:46:29, 30.90s/it]Episode [1/1]:   1%|          | 41/3742 [23:59<33:54:29, 32.98s/it]Episode [1/1]:   1%|          | 42/3742 [24:37<35:32:37, 34.58s/it]Episode [1/1]:   1%|          | 43/3742 [25:16<36:43:40, 35.75s/it]Episode [1/1]:   1%|          | 44/3742 [25:38<32:30:59, 31.65s/it]Episode [1/1]:   1%|          | 45/3742 [26:05<31:01:56, 30.22s/it]Episode [1/1]:   1%|          | 46/3742 [26:44<33:54:41, 33.03s/it]Episode [1/1]:   1%|▏         | 47/3742 [27:23<35:35:41, 34.68s/it]Episode [1/1]:   1%|▏         | 48/3742 [28:02<36:55:22, 35.98s/it]Episode [1/1]:   1%|▏         | 49/3742 [28:42<38:03:48, 37.10s/it]Episode [1/1]:   1%|▏         | 50/3742 [29:21<38:52:55, 37.91s/it]Episode [1/1]:   1%|▏         | 51/3742 [30:01<39:17:33, 38.32s/it]Episode [1/1]:   1%|▏         | 52/3742 [30:41<39:46:26, 38.80s/it]Episode [1/1]:   1%|▏         | 53/3742 [31:18<39:21:23, 38.41s/it]Episode [1/1]:   1%|▏         | 54/3742 [31:56<39:12:07, 38.27s/it]Episode [1/1]:   1%|▏         | 55/3742 [32:36<39:40:16, 38.74s/it]Episode [1/1]:   1%|▏         | 56/3742 [33:13<39:12:35, 38.30s/it]Episode [1/1]:   2%|▏         | 57/3742 [33:53<39:38:15, 38.72s/it]Episode [1/1]:   2%|▏         | 58/3742 [34:17<35:09:42, 34.36s/it]Episode [1/1]:   2%|▏         | 59/3742 [34:56<36:23:50, 35.58s/it]Episode [1/1]:   2%|▏         | 60/3742 [35:33<37:01:35, 36.20s/it]Episode [1/1]:   2%|▏         | 61/3742 [36:13<38:05:33, 37.25s/it]Episode [1/1]:   2%|▏         | 62/3742 [36:53<38:52:09, 38.02s/it]Episode [1/1]:   2%|▏         | 63/3742 [37:32<39:21:47, 38.52s/it]user

What about html and css code for a nice looking data dashboard with chartsassistant

Here is an example of HTML and CSS code for a basic data dashboard with charts using popular libraries like Chart.js and Chartist.js:

**HTML Code:**
```html
<!-- Dashboard wrapper -->
<div id="dashboard">
  <!-- Header section -->
  <header>
    <h1>Data Dashboard</h1>
  </header>
  
  <!-- Main content section -->
  <main>
    <!-- Chart 1: Line chart -->
    <section id="chart1">
      <h2>Monthly Sales</h2>
      <canvas id="chart1-canvas" width="400" height="200"></canvas>
    </section>
    
    <!-- Chart 2: Bar chart -->
    <section id="chart2">
      <h2>Top 5 Products</h2>
      <canvas id="chart2-canvas" width="400" height="200"></canvas>
    </section>
    
    <!-- Chart 3: Pie chart -->
    <section id="chart3">
      <h2>Product Categories</h2>
      <canvas id="chart3-canvas" width="400" height="200"></canvas>
    </section>
  </main>
</div>
```
**CSS Code:**
```css
/* Global styles */
body {
  font-family: Arial, sans-serif;
  margin: 0;
  padding: 0;
}

#dashboard {
  max-width: 800px;
  margin: 40px auto;
  padding: 20px;
  background-color: #f9f9f9;
  border: 1px solid #ddd;
  border-radius: 10px;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
}

header {
  background-color: #333;
  color: #fff;
  padding: 10px;
  text-align: center;
  border-bottom: 1px solid #333;
}

main {
  display: flex;
  flex-wrap: wrap;
  justify-content: center;
}

section {
  margin: 20px;
  padding: 20px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 10px;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
}

h2 {
  margin-top: 0;
}

/* Chart styles */
.canvas {


Train epoch [1/1]:   0%|          | 0/128 [00:00<?, ?it/s][A`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.

Train epoch [1/1]:   0%|          | 0/128 [00:16<?, ?it/s, pg=-0.135, rm=0.749, ret=0.749, glen=303, tlen=360, kl=0, cri=0.03, vals=0.8][A
Train epoch [1/1]:   1%|          | 1/128 [00:16<34:27, 16.28s/it, pg=-0.135, rm=0.749, ret=0.749, glen=303, tlen=360, kl=0, cri=0.03, vals=0.8][A
Train epoch [1/1]:   1%|          | 1/128 [00:22<34:27, 16.28s/it, pg=0.0982, rm=0.789, ret=0.789, glen=305, tlen=414, kl=0, cri=0.0281, vals=0.842][A
Train epoch [1/1]:   2%|▏         | 2/128 [00:22<21:26, 10.21s/it, pg=0.0982, rm=0.789, ret=0.789, glen=305, tlen=414, kl=0, cri=0.0281, vals=0.842][A
Train epoch [1/1]:   2%|▏         | 2/128 [00:28<21:26, 10.21s/it, pg=-0.0371, rm=1.13, ret=1.13, glen=383, tlen=504, kl=0, cri=0.0211, vals=1.05]  [A
Train epoch [1/1]:   2%|▏         | 3/128 [00:28<17:13,  8.27s/it, pg=-0.0371, rm=1.13, ret=1.13, glen=383, tlen=504, kl=0, cri=0.0211, vals=1.05][A
Train epoch [1/1]:   2%|▏         | 3/128 [00:34<17:13,  8.27s/it, pg=-0.473, rm=0.679, ret=0.679, glen=151, tlen=379, kl=0, cri=0.0532, vals=0.342][A
Train epoch [1/1]:   3%|▎         | 4/128 [00:34<15:06,  7.31s/it, pg=-0.473, rm=0.679, ret=0.679, glen=151, tlen=379, kl=0, cri=0.0532, vals=0.342][A
Train epoch [1/1]:   3%|▎         | 4/128 [00:40<15:06,  7.31s/it, pg=0.00177, rm=0.848, ret=0.848, glen=265, tlen=463, kl=0, cri=0.033, vals=0.386][A
Train epoch [1/1]:   4%|▍         | 5/128 [00:40<14:04,  6.86s/it, pg=0.00177, rm=0.848, ret=0.848, glen=265, tlen=463, kl=0, cri=0.033, vals=0.386][A
Train epoch [1/1]:   4%|▍         | 5/128 [00:45<14:04,  6.86s/it, pg=-0.26, rm=0.764, ret=0.764, glen=193, tlen=305, kl=0, cri=0.0392, vals=0.318] [A
Train epoch [1/1]:   5%|▍         | 6/128 [00:45<13:12,  6.50s/it, pg=-0.26, rm=0.764, ret=0.764, glen=193, tlen=305, kl=0, cri=0.0392, vals=0.318][A
Train epoch [1/1]:   5%|▍         | 6/128 [00:52<13:12,  6.50s/it, pg=0.0551, rm=0.667, ret=0.667, glen=300, tlen=506, kl=0, cri=0.0486, vals=0.58][A
Train epoch [1/1]:   5%|▌         | 7/128 [00:52<12:53,  6.39s/it, pg=0.0551, rm=0.667, ret=0.667, glen=300, tlen=506, kl=0, cri=0.0486, vals=0.58][A
Train epoch [1/1]:   5%|▌         | 7/128 [00:57<12:53,  6.39s/it, pg=-0.0223, rm=0.829, ret=0.829, glen=196, tlen=338, kl=0, cri=0.0308, vals=0.668][A
Train epoch [1/1]:   6%|▋         | 8/128 [00:57<12:28,  6.23s/it, pg=-0.0223, rm=0.829, ret=0.829, glen=196, tlen=338, kl=0, cri=0.0308, vals=0.668][A
Train epoch [1/1]:   6%|▋         | 8/128 [01:03<12:28,  6.23s/it, pg=-0.385, rm=0.718, ret=0.718, glen=236, tlen=395, kl=0, cri=0.0461, vals=0.404] [A
Train epoch [1/1]:   7%|▋         | 9/128 [01:03<12:12,  6.15s/it, pg=-0.385, rm=0.718, ret=0.718, glen=236, tlen=395, kl=0, cri=0.0461, vals=0.404][A
Train epoch [1/1]:   7%|▋         | 9/128 [01:09<12:12,  6.15s/it, pg=0.101, rm=0.744, ret=0.744, glen=273, tlen=388, kl=0, cri=0.0478, vals=0.491] [A
Train epoch [1/1]:   8%|▊         | 10/128 [01:09<11:58,  6.09s/it, pg=0.101, rm=0.744, ret=0.744, glen=273, tlen=388, kl=0, cri=0.0478, vals=0.491][A
Train epoch [1/1]:   8%|▊         | 10/128 [01:15<11:58,  6.09s/it, pg=-0.153, rm=1.31, ret=1.31, glen=381, tlen=514, kl=0, cri=0.0307, vals=0.688] [A
Train epoch [1/1]:   9%|▊         | 11/128 [01:15<11:48,  6.06s/it, pg=-0.153, rm=1.31, ret=1.31, glen=381, tlen=514, kl=0, cri=0.0307, vals=0.688][A
Train epoch [1/1]:   9%|▊         | 11/128 [01:21<11:48,  6.06s/it, pg=-0.308, rm=1, ret=1, glen=283, tlen=396, kl=0, cri=0.029, vals=0.638]       [A
Train epoch [1/1]:   9%|▉         | 12/128 [01:21<11:36,  6.01s/it, pg=-0.308, rm=1, ret=1, glen=283, tlen=396, kl=0, cri=0.029, vals=0.638][A
Train epoch [1/1]:   9%|▉         | 12/128 [01:27<11:36,  6.01s/it, pg=0.131, rm=0.93, ret=0.93, glen=269, tlen=394, kl=0, cri=0.0549, vals=0.625][A
Train epoch [1/1]:  10%|█         | 13/128 [01:27<11:30,  6.00s/it, pg=0.131, rm=0.93, ret=0.93, glen=269, tlen=394, kl=0, cri=0.0549, vals=0.625][A
Train epoch [1/1]:  10%|█         | 13/128 [01:33<11:30,  6.00s/it, pg=-0.261, rm=0.741, ret=0.741, glen=257, tlen=393, kl=0, cri=0.033, vals=0.205][A
Train epoch [1/1]:  11%|█         | 14/128 [01:33<11:27,  6.03s/it, pg=-0.261, rm=0.741, ret=0.741, glen=257, tlen=393, kl=0, cri=0.033, vals=0.205][A
Train epoch [1/1]:  11%|█         | 14/128 [01:39<11:27,  6.03s/it, pg=-0.101, rm=0.686, ret=0.686, glen=266, tlen=360, kl=0, cri=0.0348, vals=0.522][A
Train epoch [1/1]:  12%|█▏        | 15/128 [01:39<11:24,  6.06s/it, pg=-0.101, rm=0.686, ret=0.686, glen=266, tlen=360, kl=0, cri=0.0348, vals=0.522][A
Train epoch [1/1]:  12%|█▏        | 15/128 [01:55<11:24,  6.06s/it, pg=0.214, rm=0.763, ret=0.763, glen=265, tlen=542, kl=0, cri=0.0346, vals=0.494] [A
Train epoch [1/1]:  12%|█▎        | 16/128 [01:55<16:32,  8.86s/it, pg=0.214, rm=0.763, ret=0.763, glen=265, tlen=542, kl=0, cri=0.0346, vals=0.494][A
Train epoch [1/1]:  12%|█▎        | 16/128 [02:00<16:32,  8.86s/it, pg=0.534, rm=0.179, ret=0.179, glen=203, tlen=323, kl=0, cri=0.0702, vals=0.241][A
Train epoch [1/1]:  13%|█▎        | 17/128 [02:00<14:03,  7.60s/it, pg=0.534, rm=0.179, ret=0.179, glen=203, tlen=323, kl=0, cri=0.0702, vals=0.241][A
Train epoch [1/1]:  13%|█▎        | 17/128 [02:05<14:03,  7.60s/it, pg=-0.29, rm=1.41, ret=1.41, glen=368, tlen=425, kl=0, cri=0.0343, vals=0.878]  [A
Train epoch [1/1]:  14%|█▍        | 18/128 [02:05<12:59,  7.08s/it, pg=-0.29, rm=1.41, ret=1.41, glen=368, tlen=425, kl=0, cri=0.0343, vals=0.878][A
Train epoch [1/1]:  14%|█▍        | 18/128 [02:12<12:59,  7.08s/it, pg=-0.0473, rm=0.922, ret=0.922, glen=405, tlen=579, kl=0, cri=0.0286, vals=0.695][A
Train epoch [1/1]:  15%|█▍        | 19/128 [02:12<12:23,  6.82s/it, pg=-0.0473, rm=0.922, ret=0.922, glen=405, tlen=579, kl=0, cri=0.0286, vals=0.695][A
Train epoch [1/1]:  15%|█▍        | 19/128 [02:17<12:23,  6.82s/it, pg=0.0122, rm=0.725, ret=0.725, glen=270, tlen=376, kl=0, cri=0.0256, vals=0.442] [A
Train epoch [1/1]:  16%|█▌        | 20/128 [02:17<11:46,  6.54s/it, pg=0.0122, rm=0.725, ret=0.725, glen=270, tlen=376, kl=0, cri=0.0256, vals=0.442][A
Train epoch [1/1]:  16%|█▌        | 20/128 [02:24<11:46,  6.54s/it, pg=-0.208, rm=1.23, ret=1.23, glen=358, tlen=529, kl=0, cri=0.0246, vals=0.83]   [A
Train epoch [1/1]:  16%|█▋        | 21/128 [02:24<11:25,  6.41s/it, pg=-0.208, rm=1.23, ret=1.23, glen=358, tlen=529, kl=0, cri=0.0246, vals=0.83][A
Train epoch [1/1]:  16%|█▋        | 21/128 [02:30<11:25,  6.41s/it, pg=-0.144, rm=0.951, ret=0.951, glen=297, tlen=421, kl=0, cri=0.0331, vals=0.661][A
Train epoch [1/1]:  17%|█▋        | 22/128 [02:30<11:04,  6.27s/it, pg=-0.144, rm=0.951, ret=0.951, glen=297, tlen=421, kl=0, cri=0.0331, vals=0.661][A
Train epoch [1/1]:  17%|█▋        | 22/128 [02:35<11:04,  6.27s/it, pg=-0.0208, rm=0.546, ret=0.546, glen=291, tlen=376, kl=0, cri=0.0246, vals=0.58][A
Train epoch [1/1]:  18%|█▊        | 23/128 [02:35<10:47,  6.17s/it, pg=-0.0208, rm=0.546, ret=0.546, glen=291, tlen=376, kl=0, cri=0.0246, vals=0.58][A
Train epoch [1/1]:  18%|█▊        | 23/128 [02:42<10:47,  6.17s/it, pg=0.146, rm=0.816, ret=0.816, glen=342, tlen=479, kl=0, cri=0.0242, vals=0.893] [A
Train epoch [1/1]:  19%|█▉        | 24/128 [02:42<10:38,  6.14s/it, pg=0.146, rm=0.816, ret=0.816, glen=342, tlen=479, kl=0, cri=0.0242, vals=0.893][A
Train epoch [1/1]:  19%|█▉        | 24/128 [02:48<10:38,  6.14s/it, pg=-0.165, rm=1.34, ret=1.34, glen=371, tlen=510, kl=0, cri=0.029, vals=0.983]  [A
Train epoch [1/1]:  20%|█▉        | 25/128 [02:48<10:27,  6.09s/it, pg=-0.165, rm=1.34, ret=1.34, glen=371, tlen=510, kl=0, cri=0.029, vals=0.983][A
Train epoch [1/1]:  20%|█▉        | 25/128 [02:53<10:27,  6.09s/it, pg=0.0104, rm=1.09, ret=1.09, glen=439, tlen=497, kl=0, cri=0.0225, vals=0.806][A
Train epoch [1/1]:  20%|██        | 26/128 [02:53<10:17,  6.05s/it, pg=0.0104, rm=1.09, ret=1.09, glen=439, tlen=497, kl=0, cri=0.0225, vals=0.806][A
Train epoch [1/1]:  20%|██        | 26/128 [02:59<10:17,  6.05s/it, pg=0.255, rm=0.26, ret=0.26, glen=257, tlen=408, kl=0, cri=0.0346, vals=0.254] [A
Train epoch [1/1]:  21%|██        | 27/128 [02:59<10:05,  6.00s/it, pg=0.255, rm=0.26, ret=0.26, glen=257, tlen=408, kl=0, cri=0.0346, vals=0.254][A
Train epoch [1/1]:  21%|██        | 27/128 [03:06<10:05,  6.00s/it, pg=0.225, rm=0.571, ret=0.571, glen=261, tlen=366, kl=0, cri=0.0451, vals=0.501][A
Train epoch [1/1]:  22%|██▏       | 28/128 [03:06<10:07,  6.07s/it, pg=0.225, rm=0.571, ret=0.571, glen=261, tlen=366, kl=0, cri=0.0451, vals=0.501][A
Train epoch [1/1]:  22%|██▏       | 28/128 [03:12<10:07,  6.07s/it, pg=-0.0763, rm=0.696, ret=0.696, glen=330, tlen=407, kl=0, cri=0.0258, vals=0.734][A
Train epoch [1/1]:  23%|██▎       | 29/128 [03:12<09:56,  6.02s/it, pg=-0.0763, rm=0.696, ret=0.696, glen=330, tlen=407, kl=0, cri=0.0258, vals=0.734][A
Train epoch [1/1]:  23%|██▎       | 29/128 [03:17<09:56,  6.02s/it, pg=0.228, rm=0.433, ret=0.433, glen=347, tlen=462, kl=0, cri=0.0338, vals=0.564]  [A
Train epoch [1/1]:  23%|██▎       | 30/128 [03:17<09:46,  5.99s/it, pg=0.228, rm=0.433, ret=0.433, glen=347, tlen=462, kl=0, cri=0.0338, vals=0.564][A
Train epoch [1/1]:  23%|██▎       | 30/128 [03:24<09:46,  5.99s/it, pg=0.0839, rm=0.644, ret=0.644, glen=424, tlen=552, kl=0, cri=0.0221, vals=0.399][A
Train epoch [1/1]:  24%|██▍       | 31/128 [03:24<09:47,  6.05s/it, pg=0.0839, rm=0.644, ret=0.644, glen=424, tlen=552, kl=0, cri=0.0221, vals=0.399][A
Train epoch [1/1]:  24%|██▍       | 31/128 [03:38<09:47,  6.05s/it, pg=-0.133, rm=0.642, ret=0.642, glen=265, tlen=401, kl=0, cri=0.0308, vals=0.387][A
Train epoch [1/1]:  25%|██▌       | 32/128 [03:38<13:34,  8.48s/it, pg=-0.133, rm=0.642, ret=0.642, glen=265, tlen=401, kl=0, cri=0.0308, vals=0.387][A
Train epoch [1/1]:  25%|██▌       | 32/128 [03:43<13:34,  8.48s/it, pg=-0.0824, rm=0.578, ret=0.578, glen=356, tlen=493, kl=0, cri=0.03, vals=0.322] [A
Train epoch [1/1]:  26%|██▌       | 33/128 [03:43<11:46,  7.44s/it, pg=-0.0824, rm=0.578, ret=0.578, glen=356, tlen=493, kl=0, cri=0.03, vals=0.322][A
Train epoch [1/1]:  26%|██▌       | 33/128 [03:49<11:46,  7.44s/it, pg=-0.0867, rm=0.697, ret=0.697, glen=276, tlen=477, kl=0, cri=0.0367, vals=0.498][A
Train epoch [1/1]:  27%|██▋       | 34/128 [03:49<11:01,  7.04s/it, pg=-0.0867, rm=0.697, ret=0.697, glen=276, tlen=477, kl=0, cri=0.0367, vals=0.498][A
Train epoch [1/1]:  27%|██▋       | 34/128 [03:55<11:01,  7.04s/it, pg=0.0941, rm=0.989, ret=0.989, glen=348, tlen=531, kl=0, cri=0.0279, vals=0.878] [A
Train epoch [1/1]:  27%|██▋       | 35/128 [03:55<10:27,  6.74s/it, pg=0.0941, rm=0.989, ret=0.989, glen=348, tlen=531, kl=0, cri=0.0279, vals=0.878][A
Train epoch [1/1]:  27%|██▋       | 35/128 [04:01<10:27,  6.74s/it, pg=0.273, rm=1.03, ret=1.03, glen=378, tlen=512, kl=0, cri=0.0499, vals=1.13]    [A
Train epoch [1/1]:  28%|██▊       | 36/128 [04:01<10:00,  6.52s/it, pg=0.273, rm=1.03, ret=1.03, glen=378, tlen=512, kl=0, cri=0.0499, vals=1.13][A
Train epoch [1/1]:  28%|██▊       | 36/128 [04:07<10:00,  6.52s/it, pg=-0.0956, rm=1.14, ret=1.14, glen=229, tlen=322, kl=0, cri=0.0384, vals=0.667][A
Train epoch [1/1]:  29%|██▉       | 37/128 [04:07<09:31,  6.28s/it, pg=-0.0956, rm=1.14, ret=1.14, glen=229, tlen=322, kl=0, cri=0.0384, vals=0.667][A
Train epoch [1/1]:  29%|██▉       | 37/128 [04:13<09:31,  6.28s/it, pg=-0.31, rm=0.585, ret=0.585, glen=208, tlen=453, kl=0, cri=0.0353, vals=0.507][A
Train epoch [1/1]:  30%|██▉       | 38/128 [04:13<09:18,  6.20s/it, pg=-0.31, rm=0.585, ret=0.585, glen=208, tlen=453, kl=0, cri=0.0353, vals=0.507][A
Train epoch [1/1]:  30%|██▉       | 38/128 [04:19<09:18,  6.20s/it, pg=0.536, rm=0.276, ret=0.276, glen=230, tlen=520, kl=0, cri=0.0556, vals=0.605][A
Train epoch [1/1]:  30%|███       | 39/128 [04:19<09:09,  6.18s/it, pg=0.536, rm=0.276, ret=0.276, glen=230, tlen=520, kl=0, cri=0.0556, vals=0.605][A
Train epoch [1/1]:  30%|███       | 39/128 [04:25<09:09,  6.18s/it, pg=0.0484, rm=0.505, ret=0.505, glen=255, tlen=444, kl=0, cri=0.022, vals=0.445][A
Train epoch [1/1]:  31%|███▏      | 40/128 [04:25<09:04,  6.19s/it, pg=0.0484, rm=0.505, ret=0.505, glen=255, tlen=444, kl=0, cri=0.022, vals=0.445][A
Train epoch [1/1]:  31%|███▏      | 40/128 [04:31<09:04,  6.19s/it, pg=0.034, rm=0.803, ret=0.803, glen=375, tlen=554, kl=0, cri=0.0292, vals=0.341][A
Train epoch [1/1]:  32%|███▏      | 41/128 [04:31<08:55,  6.15s/it, pg=0.034, rm=0.803, ret=0.803, glen=375, tlen=554, kl=0, cri=0.0292, vals=0.341][A
Train epoch [1/1]:  32%|███▏      | 41/128 [04:37<08:55,  6.15s/it, pg=0.169, rm=0.667, ret=0.667, glen=276, tlen=498, kl=0, cri=0.0295, vals=0.755][A
Train epoch [1/1]:  33%|███▎      | 42/128 [04:37<08:51,  6.18s/it, pg=0.169, rm=0.667, ret=0.667, glen=276, tlen=498, kl=0, cri=0.0295, vals=0.755][A
Train epoch [1/1]:  33%|███▎      | 42/128 [04:43<08:51,  6.18s/it, pg=0.0151, rm=0.991, ret=0.991, glen=355, tlen=481, kl=0, cri=0.0279, vals=0.604][A
Train epoch [1/1]:  34%|███▎      | 43/128 [04:43<08:38,  6.10s/it, pg=0.0151, rm=0.991, ret=0.991, glen=355, tlen=481, kl=0, cri=0.0279, vals=0.604][A
Train epoch [1/1]:  34%|███▎      | 43/128 [04:49<08:38,  6.10s/it, pg=-0.339, rm=0.819, ret=0.819, glen=242, tlen=371, kl=0, cri=0.042, vals=0.494] [A
Train epoch [1/1]:  34%|███▍      | 44/128 [04:49<08:25,  6.02s/it, pg=-0.339, rm=0.819, ret=0.819, glen=242, tlen=371, kl=0, cri=0.042, vals=0.494][A
Train epoch [1/1]:  34%|███▍      | 44/128 [04:55<08:25,  6.02s/it, pg=-0.104, rm=0.603, ret=0.603, glen=238, tlen=398, kl=0, cri=0.0367, vals=0.495][A
Train epoch [1/1]:  35%|███▌      | 45/128 [04:55<08:17,  5.99s/it, pg=-0.104, rm=0.603, ret=0.603, glen=238, tlen=398, kl=0, cri=0.0367, vals=0.495][A
Train epoch [1/1]:  35%|███▌      | 45/128 [05:01<08:17,  5.99s/it, pg=-0.0128, rm=0.878, ret=0.878, glen=326, tlen=458, kl=0, cri=0.0288, vals=0.567][A
Train epoch [1/1]:  36%|███▌      | 46/128 [05:01<08:11,  6.00s/it, pg=-0.0128, rm=0.878, ret=0.878, glen=326, tlen=458, kl=0, cri=0.0288, vals=0.567][A
Train epoch [1/1]:  36%|███▌      | 46/128 [05:07<08:11,  6.00s/it, pg=-0.119, rm=0.62, ret=0.62, glen=302, tlen=386, kl=0, cri=0.0281, vals=0.265]   [A
Train epoch [1/1]:  37%|███▋      | 47/128 [05:07<08:07,  6.02s/it, pg=-0.119, rm=0.62, ret=0.62, glen=302, tlen=386, kl=0, cri=0.0281, vals=0.265][A
Train epoch [1/1]:  37%|███▋      | 47/128 [05:21<08:07,  6.02s/it, pg=-0.0989, rm=0.728, ret=0.728, glen=387, tlen=557, kl=0, cri=0.0266, vals=0.734][A
Train epoch [1/1]:  38%|███▊      | 48/128 [05:21<11:01,  8.27s/it, pg=-0.0989, rm=0.728, ret=0.728, glen=387, tlen=557, kl=0, cri=0.0266, vals=0.734][A
Train epoch [1/1]:  38%|███▊      | 48/128 [05:25<11:01,  8.27s/it, pg=0.0799, rm=0.696, ret=0.696, glen=250, tlen=444, kl=0, cri=0.0263, vals=0.746] [A
Train epoch [1/1]:  38%|███▊      | 49/128 [05:25<09:30,  7.22s/it, pg=0.0799, rm=0.696, ret=0.696, glen=250, tlen=444, kl=0, cri=0.0263, vals=0.746][A
Train epoch [1/1]:  38%|███▊      | 49/128 [05:31<09:30,  7.22s/it, pg=-0.0779, rm=0.927, ret=0.927, glen=326, tlen=399, kl=0, cri=0.0292, vals=0.854][A
Train epoch [1/1]:  39%|███▉      | 50/128 [05:31<08:51,  6.81s/it, pg=-0.0779, rm=0.927, ret=0.927, glen=326, tlen=399, kl=0, cri=0.0292, vals=0.854][A
Train epoch [1/1]:  39%|███▉      | 50/128 [05:37<08:51,  6.81s/it, pg=-0.0312, rm=0.742, ret=0.742, glen=311, tlen=401, kl=0, cri=0.0335, vals=0.492][A
Train epoch [1/1]:  40%|███▉      | 51/128 [05:37<08:25,  6.56s/it, pg=-0.0312, rm=0.742, ret=0.742, glen=311, tlen=401, kl=0, cri=0.0335, vals=0.492][A
Train epoch [1/1]:  40%|███▉      | 51/128 [05:43<08:25,  6.56s/it, pg=0.0592, rm=0.219, ret=0.219, glen=304, tlen=422, kl=0, cri=0.0333, vals=0.142] [A
Train epoch [1/1]:  41%|████      | 52/128 [05:43<08:09,  6.45s/it, pg=0.0592, rm=0.219, ret=0.219, glen=304, tlen=422, kl=0, cri=0.0333, vals=0.142][A
Train epoch [1/1]:  41%|████      | 52/128 [05:49<08:09,  6.45s/it, pg=-0.223, rm=0.444, ret=0.444, glen=276, tlen=401, kl=0, cri=0.0382, vals=0.467][A
Train epoch [1/1]:  41%|████▏     | 53/128 [05:49<07:50,  6.28s/it, pg=-0.223, rm=0.444, ret=0.444, glen=276, tlen=401, kl=0, cri=0.0382, vals=0.467][A
Train epoch [1/1]:  41%|████▏     | 53/128 [05:56<07:50,  6.28s/it, pg=0.12, rm=0.788, ret=0.788, glen=343, tlen=472, kl=0, cri=0.0278, vals=0.772]  [A
Train epoch [1/1]:  42%|████▏     | 54/128 [05:56<07:43,  6.27s/it, pg=0.12, rm=0.788, ret=0.788, glen=343, tlen=472, kl=0, cri=0.0278, vals=0.772][A
Train epoch [1/1]:  42%|████▏     | 54/128 [06:01<07:43,  6.27s/it, pg=0.0209, rm=0.703, ret=0.703, glen=321, tlen=443, kl=0, cri=0.0219, vals=0.539][A
Train epoch [1/1]:  43%|████▎     | 55/128 [06:01<07:30,  6.17s/it, pg=0.0209, rm=0.703, ret=0.703, glen=321, tlen=443, kl=0, cri=0.0219, vals=0.539][A
Train epoch [1/1]:  43%|████▎     | 55/128 [06:07<07:30,  6.17s/it, pg=0.0289, rm=0.971, ret=0.971, glen=375, tlen=460, kl=0, cri=0.0224, vals=0.705][A
Train epoch [1/1]:  44%|████▍     | 56/128 [06:07<07:18,  6.10s/it, pg=0.0289, rm=0.971, ret=0.971, glen=375, tlen=460, kl=0, cri=0.0224, vals=0.705][A
Train epoch [1/1]:  44%|████▍     | 56/128 [06:13<07:18,  6.10s/it, pg=-0.0815, rm=1.03, ret=1.03, glen=266, tlen=423, kl=0, cri=0.0315, vals=0.764] [A
Train epoch [1/1]:  45%|████▍     | 57/128 [06:13<07:10,  6.06s/it, pg=-0.0815, rm=1.03, ret=1.03, glen=266, tlen=423, kl=0, cri=0.0315, vals=0.764][A
Train epoch [1/1]:  45%|████▍     | 57/128 [06:20<07:10,  6.06s/it, pg=0.0484, rm=1.16, ret=1.16, glen=297, tlen=415, kl=0, cri=0.021, vals=0.921]  [A
Train epoch [1/1]:  45%|████▌     | 58/128 [06:20<07:06,  6.10s/it, pg=0.0484, rm=1.16, ret=1.16, glen=297, tlen=415, kl=0, cri=0.021, vals=0.921][A
Train epoch [1/1]:  45%|████▌     | 58/128 [06:25<07:06,  6.10s/it, pg=0.0358, rm=0.681, ret=0.681, glen=191, tlen=388, kl=0, cri=0.0374, vals=0.728][A
Train epoch [1/1]:  46%|████▌     | 59/128 [06:25<06:56,  6.03s/it, pg=0.0358, rm=0.681, ret=0.681, glen=191, tlen=388, kl=0, cri=0.0374, vals=0.728][A
Train epoch [1/1]:  46%|████▌     | 59/128 [06:31<06:56,  6.03s/it, pg=0.564, rm=0.662, ret=0.662, glen=214, tlen=298, kl=0, cri=0.126, vals=0.548]  [A
Train epoch [1/1]:  47%|████▋     | 60/128 [06:31<06:47,  5.99s/it, pg=0.564, rm=0.662, ret=0.662, glen=214, tlen=298, kl=0, cri=0.126, vals=0.548][A
Train epoch [1/1]:  47%|████▋     | 60/128 [06:37<06:47,  5.99s/it, pg=-0.0114, rm=1.42, ret=1.42, glen=413, tlen=460, kl=0, cri=0.0204, vals=1.03][A
Train epoch [1/1]:  48%|████▊     | 61/128 [06:37<06:39,  5.97s/it, pg=-0.0114, rm=1.42, ret=1.42, glen=413, tlen=460, kl=0, cri=0.0204, vals=1.03][A
Train epoch [1/1]:  48%|████▊     | 61/128 [06:43<06:39,  5.97s/it, pg=0.239, rm=0.844, ret=0.844, glen=260, tlen=409, kl=0, cri=0.0289, vals=0.817][A
Train epoch [1/1]:  48%|████▊     | 62/128 [06:43<06:31,  5.94s/it, pg=0.239, rm=0.844, ret=0.844, glen=260, tlen=409, kl=0, cri=0.0289, vals=0.817][A
Train epoch [1/1]:  48%|████▊     | 62/128 [06:49<06:31,  5.94s/it, pg=-0.0658, rm=1.34, ret=1.34, glen=390, tlen=472, kl=0, cri=0.0226, vals=0.76] [A
Train epoch [1/1]:  49%|████▉     | 63/128 [06:49<06:28,  5.97s/it, pg=-0.0658, rm=1.34, ret=1.34, glen=390, tlen=472, kl=0, cri=0.0226, vals=0.76][A
Train epoch [1/1]:  49%|████▉     | 63/128 [07:02<06:28,  5.97s/it, pg=-0.175, rm=0.591, ret=0.591, glen=254, tlen=394, kl=0, cri=0.0389, vals=0.409][A
Train epoch [1/1]:  50%|█████     | 64/128 [07:02<08:43,  8.19s/it, pg=-0.175, rm=0.591, ret=0.591, glen=254, tlen=394, kl=0, cri=0.0389, vals=0.409][A
Train epoch [1/1]:  50%|█████     | 64/128 [07:07<08:43,  8.19s/it, pg=0.00178, rm=0.536, ret=0.536, glen=364, tlen=486, kl=0, cri=0.0223, vals=0.541][A
Train epoch [1/1]:  51%|█████     | 65/128 [07:07<07:31,  7.17s/it, pg=0.00178, rm=0.536, ret=0.536, glen=364, tlen=486, kl=0, cri=0.0223, vals=0.541][A
Train epoch [1/1]:  51%|█████     | 65/128 [07:14<07:31,  7.17s/it, pg=0.706, rm=0.585, ret=0.585, glen=288, tlen=399, kl=0, cri=0.118, vals=1.12]    [A
Train epoch [1/1]:  52%|█████▏    | 66/128 [07:14<07:06,  6.88s/it, pg=0.706, rm=0.585, ret=0.585, glen=288, tlen=399, kl=0, cri=0.118, vals=1.12][A
Train epoch [1/1]:  52%|█████▏    | 66/128 [07:19<07:06,  6.88s/it, pg=-0.204, rm=0.855, ret=0.855, glen=333, tlen=447, kl=0, cri=0.0335, vals=0.676][A
Train epoch [1/1]:  52%|█████▏    | 67/128 [07:19<06:42,  6.60s/it, pg=-0.204, rm=0.855, ret=0.855, glen=333, tlen=447, kl=0, cri=0.0335, vals=0.676][A
Train epoch [1/1]:  52%|█████▏    | 67/128 [07:25<06:42,  6.60s/it, pg=-0.191, rm=0.797, ret=0.797, glen=207, tlen=347, kl=0, cri=0.0344, vals=0.806][A
Train epoch [1/1]:  53%|█████▎    | 68/128 [07:25<06:22,  6.38s/it, pg=-0.191, rm=0.797, ret=0.797, glen=207, tlen=347, kl=0, cri=0.0344, vals=0.806][A
Train epoch [1/1]:  53%|█████▎    | 68/128 [07:31<06:22,  6.38s/it, pg=0.17, rm=0.548, ret=0.548, glen=200, tlen=270, kl=0, cri=0.0408, vals=0.368]  [A
Train epoch [1/1]:  54%|█████▍    | 69/128 [07:31<06:04,  6.17s/it, pg=0.17, rm=0.548, ret=0.548, glen=200, tlen=270, kl=0, cri=0.0408, vals=0.368][A
Train epoch [1/1]:  54%|█████▍    | 69/128 [07:37<06:04,  6.17s/it, pg=-0.0135, rm=0.782, ret=0.782, glen=269, tlen=299, kl=0, cri=0.0234, vals=0.441][A
Train epoch [1/1]:  55%|█████▍    | 70/128 [07:37<05:51,  6.06s/it, pg=-0.0135, rm=0.782, ret=0.782, glen=269, tlen=299, kl=0, cri=0.0234, vals=0.441][A
Train epoch [1/1]:  55%|█████▍    | 70/128 [07:43<05:51,  6.06s/it, pg=-0.225, rm=0.533, ret=0.533, glen=306, tlen=484, kl=0, cri=0.0488, vals=0.515] [A
Train epoch [1/1]:  55%|█████▌    | 71/128 [07:43<05:45,  6.06s/it, pg=-0.225, rm=0.533, ret=0.533, glen=306, tlen=484, kl=0, cri=0.0488, vals=0.515][A
Train epoch [1/1]:  55%|█████▌    | 71/128 [07:49<05:45,  6.06s/it, pg=-0.177, rm=1.24, ret=1.24, glen=357, tlen=440, kl=0, cri=0.0224, vals=0.968]  [A
Train epoch [1/1]:  56%|█████▋    | 72/128 [07:49<05:40,  6.08s/it, pg=-0.177, rm=1.24, ret=1.24, glen=357, tlen=440, kl=0, cri=0.0224, vals=0.968][A
Train epoch [1/1]:  56%|█████▋    | 72/128 [07:55<05:40,  6.08s/it, pg=0.0977, rm=0.953, ret=0.953, glen=329, tlen=423, kl=0, cri=0.0271, vals=0.921][A
Train epoch [1/1]:  57%|█████▋    | 73/128 [07:55<05:32,  6.04s/it, pg=0.0977, rm=0.953, ret=0.953, glen=329, tlen=423, kl=0, cri=0.0271, vals=0.921][A
Train epoch [1/1]:  57%|█████▋    | 73/128 [08:01<05:32,  6.04s/it, pg=-0.403, rm=0.83, ret=0.83, glen=266, tlen=412, kl=0, cri=0.0422, vals=0.768]  [A
Train epoch [1/1]:  58%|█████▊    | 74/128 [08:01<05:27,  6.07s/it, pg=-0.403, rm=0.83, ret=0.83, glen=266, tlen=412, kl=0, cri=0.0422, vals=0.768][A
Train epoch [1/1]:  58%|█████▊    | 74/128 [08:07<05:27,  6.07s/it, pg=0.0141, rm=0.723, ret=0.723, glen=300, tlen=432, kl=0, cri=0.0285, vals=0.651][A
Train epoch [1/1]:  59%|█████▊    | 75/128 [08:07<05:22,  6.09s/it, pg=0.0141, rm=0.723, ret=0.723, glen=300, tlen=432, kl=0, cri=0.0285, vals=0.651][A
Train epoch [1/1]:  59%|█████▊    | 75/128 [08:13<05:22,  6.09s/it, pg=-0.0675, rm=1.31, ret=1.31, glen=379, tlen=532, kl=0, cri=0.023, vals=1.04]   [A
Train epoch [1/1]:  59%|█████▉    | 76/128 [08:13<05:14,  6.05s/it, pg=-0.0675, rm=1.31, ret=1.31, glen=379, tlen=532, kl=0, cri=0.023, vals=1.04][A
Train epoch [1/1]:  59%|█████▉    | 76/128 [08:20<05:14,  6.05s/it, pg=0.149, rm=0.851, ret=0.851, glen=439, tlen=570, kl=0, cri=0.0259, vals=0.973][A
Train epoch [1/1]:  60%|██████    | 77/128 [08:20<05:12,  6.14s/it, pg=0.149, rm=0.851, ret=0.851, glen=439, tlen=570, kl=0, cri=0.0259, vals=0.973][A
Train epoch [1/1]:  60%|██████    | 77/128 [08:25<05:12,  6.14s/it, pg=-0.0502, rm=0.773, ret=0.773, glen=326, tlen=443, kl=0, cri=0.0239, vals=0.592][A
Train epoch [1/1]:  61%|██████    | 78/128 [08:25<05:03,  6.07s/it, pg=-0.0502, rm=0.773, ret=0.773, glen=326, tlen=443, kl=0, cri=0.0239, vals=0.592][A
Train epoch [1/1]:  61%|██████    | 78/128 [08:31<05:03,  6.07s/it, pg=-0.0909, rm=0.492, ret=0.492, glen=176, tlen=336, kl=0, cri=0.0327, vals=0.437][A
Train epoch [1/1]:  62%|██████▏   | 79/128 [08:31<04:53,  5.99s/it, pg=-0.0909, rm=0.492, ret=0.492, glen=176, tlen=336, kl=0, cri=0.0327, vals=0.437][A
Train epoch [1/1]:  62%|██████▏   | 79/128 [08:45<04:53,  5.99s/it, pg=0.213, rm=0.925, ret=0.925, glen=290, tlen=432, kl=0, cri=0.0501, vals=0.871]  [A
Train epoch [1/1]:  62%|██████▎   | 80/128 [08:45<06:33,  8.21s/it, pg=0.213, rm=0.925, ret=0.925, glen=290, tlen=432, kl=0, cri=0.0501, vals=0.871][A
Train epoch [1/1]:  62%|██████▎   | 80/128 [08:49<06:33,  8.21s/it, pg=0.0636, rm=1.12, ret=1.12, glen=335, tlen=466, kl=0, cri=0.047, vals=1.02]   [A
Train epoch [1/1]:  63%|██████▎   | 81/128 [08:49<05:36,  7.17s/it, pg=0.0636, rm=1.12, ret=1.12, glen=335, tlen=466, kl=0, cri=0.047, vals=1.02][A
Train epoch [1/1]:  63%|██████▎   | 81/128 [08:55<05:36,  7.17s/it, pg=0.298, rm=0.49, ret=0.49, glen=173, tlen=313, kl=0, cri=0.0799, vals=0.439][A
Train epoch [1/1]:  64%|██████▍   | 82/128 [08:55<05:11,  6.77s/it, pg=0.298, rm=0.49, ret=0.49, glen=173, tlen=313, kl=0, cri=0.0799, vals=0.439][A
Train epoch [1/1]:  64%|██████▍   | 82/128 [09:01<05:11,  6.77s/it, pg=-0.0179, rm=1.15, ret=1.15, glen=385, tlen=454, kl=0, cri=0.0308, vals=0.915][A
Train epoch [1/1]:  65%|██████▍   | 83/128 [09:01<04:54,  6.54s/it, pg=-0.0179, rm=1.15, ret=1.15, glen=385, tlen=454, kl=0, cri=0.0308, vals=0.915][A
Train epoch [1/1]:  65%|██████▍   | 83/128 [09:07<04:54,  6.54s/it, pg=-0.0369, rm=0.977, ret=0.977, glen=384, tlen=492, kl=0, cri=0.0288, vals=0.89][A
Train epoch [1/1]:  66%|██████▌   | 84/128 [09:07<04:42,  6.43s/it, pg=-0.0369, rm=0.977, ret=0.977, glen=384, tlen=492, kl=0, cri=0.0288, vals=0.89][A
Train epoch [1/1]:  66%|██████▌   | 84/128 [09:14<04:42,  6.43s/it, pg=-0.185, rm=0.556, ret=0.556, glen=179, tlen=435, kl=0, cri=0.0768, vals=0.795][A
Train epoch [1/1]:  66%|██████▋   | 85/128 [09:14<04:32,  6.34s/it, pg=-0.185, rm=0.556, ret=0.556, glen=179, tlen=435, kl=0, cri=0.0768, vals=0.795][A
Train epoch [1/1]:  66%|██████▋   | 85/128 [09:19<04:32,  6.34s/it, pg=0.166, rm=0.518, ret=0.518, glen=195, tlen=280, kl=0, cri=0.0501, vals=0.773] [A
Train epoch [1/1]:  67%|██████▋   | 86/128 [09:19<04:18,  6.16s/it, pg=0.166, rm=0.518, ret=0.518, glen=195, tlen=280, kl=0, cri=0.0501, vals=0.773][A
Train epoch [1/1]:  67%|██████▋   | 86/128 [09:25<04:18,  6.16s/it, pg=-0.111, rm=1.52, ret=1.52, glen=342, tlen=540, kl=0, cri=0.0276, vals=1.14]  [A
Train epoch [1/1]:  68%|██████▊   | 87/128 [09:25<04:12,  6.17s/it, pg=-0.111, rm=1.52, ret=1.52, glen=342, tlen=540, kl=0, cri=0.0276, vals=1.14][A
Train epoch [1/1]:  68%|██████▊   | 87/128 [09:32<04:12,  6.17s/it, pg=0.276, rm=0.858, ret=0.858, glen=346, tlen=413, kl=0, cri=0.067, vals=0.976][A
Train epoch [1/1]:  69%|██████▉   | 88/128 [09:32<04:06,  6.15s/it, pg=0.276, rm=0.858, ret=0.858, glen=346, tlen=413, kl=0, cri=0.067, vals=0.976][A
Train epoch [1/1]:  69%|██████▉   | 88/128 [09:38<04:06,  6.15s/it, pg=0.0363, rm=1.21, ret=1.21, glen=263, tlen=485, kl=0, cri=0.0472, vals=1]    [A
Train epoch [1/1]:  70%|██████▉   | 89/128 [09:38<03:58,  6.13s/it, pg=0.0363, rm=1.21, ret=1.21, glen=263, tlen=485, kl=0, cri=0.0472, vals=1][A
Train epoch [1/1]:  70%|██████▉   | 89/128 [09:44<03:58,  6.13s/it, pg=0.0124, rm=1.26, ret=1.26, glen=455, tlen=538, kl=0, cri=0.0262, vals=1.08][A
Train epoch [1/1]:  70%|███████   | 90/128 [09:44<03:51,  6.09s/it, pg=0.0124, rm=1.26, ret=1.26, glen=455, tlen=538, kl=0, cri=0.0262, vals=1.08][A
Train epoch [1/1]:  70%|███████   | 90/128 [09:50<03:51,  6.09s/it, pg=-0.125, rm=1.08, ret=1.08, glen=265, tlen=453, kl=0, cri=0.0281, vals=0.746][A
Train epoch [1/1]:  71%|███████   | 91/128 [09:50<03:46,  6.12s/it, pg=-0.125, rm=1.08, ret=1.08, glen=265, tlen=453, kl=0, cri=0.0281, vals=0.746][A
Train epoch [1/1]:  71%|███████   | 91/128 [09:56<03:46,  6.12s/it, pg=0.0427, rm=1.13, ret=1.13, glen=365, tlen=458, kl=0, cri=0.0255, vals=1.04] [A
Train epoch [1/1]:  72%|███████▏  | 92/128 [09:56<03:39,  6.10s/it, pg=0.0427, rm=1.13, ret=1.13, glen=365, tlen=458, kl=0, cri=0.0255, vals=1.04][A
Train epoch [1/1]:  72%|███████▏  | 92/128 [10:02<03:39,  6.10s/it, pg=-0.197, rm=1.2, ret=1.2, glen=354, tlen=564, kl=0, cri=0.0311, vals=0.715] [A
Train epoch [1/1]:  73%|███████▎  | 93/128 [10:02<03:33,  6.09s/it, pg=-0.197, rm=1.2, ret=1.2, glen=354, tlen=564, kl=0, cri=0.0311, vals=0.715][A
Train epoch [1/1]:  73%|███████▎  | 93/128 [10:08<03:33,  6.09s/it, pg=-0.299, rm=1.04, ret=1.04, glen=310, tlen=461, kl=0, cri=0.0454, vals=0.65][A
Train epoch [1/1]:  73%|███████▎  | 94/128 [10:08<03:24,  6.03s/it, pg=-0.299, rm=1.04, ret=1.04, glen=310, tlen=461, kl=0, cri=0.0454, vals=0.65][A
Train epoch [1/1]:  73%|███████▎  | 94/128 [10:14<03:24,  6.03s/it, pg=0.108, rm=0.29, ret=0.29, glen=234, tlen=313, kl=0, cri=0.049, vals=0.215] [A
Train epoch [1/1]:  74%|███████▍  | 95/128 [10:14<03:17,  5.98s/it, pg=0.108, rm=0.29, ret=0.29, glen=234, tlen=313, kl=0, cri=0.049, vals=0.215][A
Train epoch [1/1]:  74%|███████▍  | 95/128 [10:27<03:17,  5.98s/it, pg=-0.142, rm=0.582, ret=0.582, glen=186, tlen=307, kl=0, cri=0.0442, vals=0.394][A
Train epoch [1/1]:  75%|███████▌  | 96/128 [10:27<04:20,  8.13s/it, pg=-0.142, rm=0.582, ret=0.582, glen=186, tlen=307, kl=0, cri=0.0442, vals=0.394][A
Train epoch [1/1]:  75%|███████▌  | 96/128 [10:32<04:20,  8.13s/it, pg=-0.0606, rm=0.64, ret=0.64, glen=295, tlen=418, kl=0, cri=0.0471, vals=0.979] [A
Train epoch [1/1]:  76%|███████▌  | 97/128 [10:32<03:40,  7.10s/it, pg=-0.0606, rm=0.64, ret=0.64, glen=295, tlen=418, kl=0, cri=0.0471, vals=0.979][A
Train epoch [1/1]:  76%|███████▌  | 97/128 [10:37<03:40,  7.10s/it, pg=0.335, rm=0.557, ret=0.557, glen=207, tlen=381, kl=0, cri=0.0787, vals=0.681][A
Train epoch [1/1]:  77%|███████▋  | 98/128 [10:37<03:20,  6.69s/it, pg=0.335, rm=0.557, ret=0.557, glen=207, tlen=381, kl=0, cri=0.0787, vals=0.681][A
Train epoch [1/1]:  77%|███████▋  | 98/128 [10:43<03:20,  6.69s/it, pg=0.139, rm=0.488, ret=0.488, glen=218, tlen=457, kl=0, cri=0.0607, vals=0.788][A
Train epoch [1/1]:  77%|███████▋  | 99/128 [10:43<03:08,  6.51s/it, pg=0.139, rm=0.488, ret=0.488, glen=218, tlen=457, kl=0, cri=0.0607, vals=0.788][A
Train epoch [1/1]:  77%|███████▋  | 99/128 [10:49<03:08,  6.51s/it, pg=-0.174, rm=0.936, ret=0.936, glen=317, tlen=482, kl=0, cri=0.0475, vals=0.89][A
Train epoch [1/1]:  78%|███████▊  | 100/128 [10:49<02:57,  6.32s/it, pg=-0.174, rm=0.936, ret=0.936, glen=317, tlen=482, kl=0, cri=0.0475, vals=0.89][A
Train epoch [1/1]:  78%|███████▊  | 100/128 [10:55<02:57,  6.32s/it, pg=0.386, rm=0.587, ret=0.587, glen=330, tlen=476, kl=0, cri=0.1, vals=0.92]    [A
Train epoch [1/1]:  79%|███████▉  | 101/128 [10:55<02:47,  6.22s/it, pg=0.386, rm=0.587, ret=0.587, glen=330, tlen=476, kl=0, cri=0.1, vals=0.92][A
Train epoch [1/1]:  79%|███████▉  | 101/128 [11:01<02:47,  6.22s/it, pg=-0.138, rm=0.892, ret=0.892, glen=216, tlen=379, kl=0, cri=0.057, vals=0.913][A
Train epoch [1/1]:  80%|███████▉  | 102/128 [11:01<02:38,  6.11s/it, pg=-0.138, rm=0.892, ret=0.892, glen=216, tlen=379, kl=0, cri=0.057, vals=0.913][A
Train epoch [1/1]:  80%|███████▉  | 102/128 [11:07<02:38,  6.11s/it, pg=0.218, rm=0.723, ret=0.723, glen=356, tlen=616, kl=0, cri=0.0415, vals=1.13] [A
Train epoch [1/1]:  80%|████████  | 103/128 [11:07<02:34,  6.16s/it, pg=0.218, rm=0.723, ret=0.723, glen=356, tlen=616, kl=0, cri=0.0415, vals=1.13][A
Train epoch [1/1]:  80%|████████  | 103/128 [11:13<02:34,  6.16s/it, pg=0.234, rm=0.674, ret=0.674, glen=276, tlen=451, kl=0, cri=0.104, vals=0.907][A
Train epoch [1/1]:  81%|████████▏ | 104/128 [11:13<02:27,  6.15s/it, pg=0.234, rm=0.674, ret=0.674, glen=276, tlen=451, kl=0, cri=0.104, vals=0.907][A
Train epoch [1/1]:  81%|████████▏ | 104/128 [11:19<02:27,  6.15s/it, pg=-0.0484, rm=0.696, ret=0.696, glen=317, tlen=413, kl=0, cri=0.0384, vals=0.779][A
Train epoch [1/1]:  82%|████████▏ | 105/128 [11:19<02:20,  6.09s/it, pg=-0.0484, rm=0.696, ret=0.696, glen=317, tlen=413, kl=0, cri=0.0384, vals=0.779][A
Train epoch [1/1]:  82%|████████▏ | 105/128 [11:26<02:20,  6.09s/it, pg=-0.07, rm=1.34, ret=1.34, glen=425, tlen=490, kl=0, cri=0.0244, vals=1.21]     [A
Train epoch [1/1]:  83%|████████▎ | 106/128 [11:26<02:13,  6.09s/it, pg=-0.07, rm=1.34, ret=1.34, glen=425, tlen=490, kl=0, cri=0.0244, vals=1.21][A
Train epoch [1/1]:  83%|████████▎ | 106/128 [11:32<02:13,  6.09s/it, pg=-0.0841, rm=1.07, ret=1.07, glen=380, tlen=518, kl=0, cri=0.0363, vals=0.841][A
Train epoch [1/1]:  84%|████████▎ | 107/128 [11:32<02:08,  6.10s/it, pg=-0.0841, rm=1.07, ret=1.07, glen=380, tlen=518, kl=0, cri=0.0363, vals=0.841][A
Train epoch [1/1]:  84%|████████▎ | 107/128 [11:38<02:08,  6.10s/it, pg=-0.0911, rm=0.99, ret=0.99, glen=289, tlen=428, kl=0, cri=0.0445, vals=0.787][A
Train epoch [1/1]:  84%|████████▍ | 108/128 [11:38<02:01,  6.07s/it, pg=-0.0911, rm=0.99, ret=0.99, glen=289, tlen=428, kl=0, cri=0.0445, vals=0.787][A
Train epoch [1/1]:  84%|████████▍ | 108/128 [11:43<02:01,  6.07s/it, pg=-0.211, rm=0.572, ret=0.572, glen=148, tlen=312, kl=0, cri=0.0541, vals=0.512][A
Train epoch [1/1]:  85%|████████▌ | 109/128 [11:43<01:53,  5.99s/it, pg=-0.211, rm=0.572, ret=0.572, glen=148, tlen=312, kl=0, cri=0.0541, vals=0.512][A
Train epoch [1/1]:  85%|████████▌ | 109/128 [11:49<01:53,  5.99s/it, pg=-0.0457, rm=0.953, ret=0.953, glen=298, tlen=462, kl=0, cri=0.0469, vals=0.909][A
Train epoch [1/1]:  86%|████████▌ | 110/128 [11:49<01:48,  6.01s/it, pg=-0.0457, rm=0.953, ret=0.953, glen=298, tlen=462, kl=0, cri=0.0469, vals=0.909][A
Train epoch [1/1]:  86%|████████▌ | 110/128 [11:55<01:48,  6.01s/it, pg=-0.111, rm=0.834, ret=0.834, glen=313, tlen=462, kl=0, cri=0.0469, vals=0.911] [A
Train epoch [1/1]:  87%|████████▋ | 111/128 [11:55<01:41,  6.00s/it, pg=-0.111, rm=0.834, ret=0.834, glen=313, tlen=462, kl=0, cri=0.0469, vals=0.911][A
Train epoch [1/1]:  87%|████████▋ | 111/128 [12:09<01:41,  6.00s/it, pg=0.00832, rm=0.868, ret=0.868, glen=383, tlen=472, kl=0, cri=0.0367, vals=1.07][A
Train epoch [1/1]:  88%|████████▊ | 112/128 [12:09<02:10,  8.16s/it, pg=0.00832, rm=0.868, ret=0.868, glen=383, tlen=472, kl=0, cri=0.0367, vals=1.07][A
Train epoch [1/1]:  88%|████████▊ | 112/128 [12:13<02:10,  8.16s/it, pg=-0.0631, rm=0.754, ret=0.754, glen=227, tlen=461, kl=0, cri=0.0737, vals=0.867][A
Train epoch [1/1]:  88%|████████▊ | 113/128 [12:13<01:47,  7.15s/it, pg=-0.0631, rm=0.754, ret=0.754, glen=227, tlen=461, kl=0, cri=0.0737, vals=0.867][A
Train epoch [1/1]:  88%|████████▊ | 113/128 [12:19<01:47,  7.15s/it, pg=0.0605, rm=0.616, ret=0.616, glen=320, tlen=456, kl=0, cri=0.0564, vals=0.917] [A
Train epoch [1/1]:  89%|████████▉ | 114/128 [12:19<01:35,  6.80s/it, pg=0.0605, rm=0.616, ret=0.616, glen=320, tlen=456, kl=0, cri=0.0564, vals=0.917][A
Train epoch [1/1]:  89%|████████▉ | 114/128 [12:25<01:35,  6.80s/it, pg=-0.418, rm=0.671, ret=0.671, glen=335, tlen=416, kl=0, cri=0.0848, vals=0.604][A
Train epoch [1/1]:  90%|████████▉ | 115/128 [12:25<01:25,  6.56s/it, pg=-0.418, rm=0.671, ret=0.671, glen=335, tlen=416, kl=0, cri=0.0848, vals=0.604][A
Train epoch [1/1]:  90%|████████▉ | 115/128 [12:31<01:25,  6.56s/it, pg=0.0333, rm=0.451, ret=0.451, glen=320, tlen=427, kl=0, cri=0.0766, vals=0.754][A
Train epoch [1/1]:  91%|█████████ | 116/128 [12:31<01:16,  6.37s/it, pg=0.0333, rm=0.451, ret=0.451, glen=320, tlen=427, kl=0, cri=0.0766, vals=0.754][A
Train epoch [1/1]:  91%|█████████ | 116/128 [12:38<01:16,  6.37s/it, pg=-0.0198, rm=0.756, ret=0.756, glen=325, tlen=452, kl=0, cri=0.0696, vals=0.808][A
Train epoch [1/1]:  91%|█████████▏| 117/128 [12:38<01:09,  6.31s/it, pg=-0.0198, rm=0.756, ret=0.756, glen=325, tlen=452, kl=0, cri=0.0696, vals=0.808][A
Train epoch [1/1]:  91%|█████████▏| 117/128 [12:44<01:09,  6.31s/it, pg=-0.0556, rm=1.02, ret=1.02, glen=384, tlen=537, kl=0, cri=0.0557, vals=0.922]  [A
Train epoch [1/1]:  92%|█████████▏| 118/128 [12:44<01:02,  6.27s/it, pg=-0.0556, rm=1.02, ret=1.02, glen=384, tlen=537, kl=0, cri=0.0557, vals=0.922][A
Train epoch [1/1]:  92%|█████████▏| 118/128 [12:50<01:02,  6.27s/it, pg=-0.107, rm=1.09, ret=1.09, glen=296, tlen=364, kl=0, cri=0.0692, vals=0.891] [A
Train epoch [1/1]:  93%|█████████▎| 119/128 [12:50<00:55,  6.14s/it, pg=-0.107, rm=1.09, ret=1.09, glen=296, tlen=364, kl=0, cri=0.0692, vals=0.891][A
Train epoch [1/1]:  93%|█████████▎| 119/128 [12:55<00:55,  6.14s/it, pg=-0.387, rm=1.05, ret=1.05, glen=308, tlen=409, kl=0, cri=0.051, vals=1.05]  [A
Train epoch [1/1]:  94%|█████████▍| 120/128 [12:55<00:48,  6.07s/it, pg=-0.387, rm=1.05, ret=1.05, glen=308, tlen=409, kl=0, cri=0.051, vals=1.05][A
Train epoch [1/1]:  94%|█████████▍| 120/128 [13:02<00:48,  6.07s/it, pg=0.364, rm=0.441, ret=0.441, glen=302, tlen=574, kl=0, cri=0.098, vals=0.817][A
Train epoch [1/1]:  95%|█████████▍| 121/128 [13:02<00:43,  6.21s/it, pg=0.364, rm=0.441, ret=0.441, glen=302, tlen=574, kl=0, cri=0.098, vals=0.817][A
Train epoch [1/1]:  95%|█████████▍| 121/128 [13:08<00:43,  6.21s/it, pg=-0.658, rm=1.36, ret=1.36, glen=244, tlen=382, kl=0, cri=0.0517, vals=1.06] [A
Train epoch [1/1]:  95%|█████████▌| 122/128 [13:08<00:37,  6.18s/it, pg=-0.658, rm=1.36, ret=1.36, glen=244, tlen=382, kl=0, cri=0.0517, vals=1.06][A
Train epoch [1/1]:  95%|█████████▌| 122/128 [13:14<00:37,  6.18s/it, pg=-0.276, rm=0.752, ret=0.752, glen=289, tlen=370, kl=0, cri=0.0712, vals=0.759][A
Train epoch [1/1]:  96%|█████████▌| 123/128 [13:14<00:30,  6.08s/it, pg=-0.276, rm=0.752, ret=0.752, glen=289, tlen=370, kl=0, cri=0.0712, vals=0.759][A
Train epoch [1/1]:  96%|█████████▌| 123/128 [13:20<00:30,  6.08s/it, pg=-0.275, rm=1.34, ret=1.34, glen=355, tlen=474, kl=0, cri=0.0367, vals=1.25]   [A
Train epoch [1/1]:  97%|█████████▋| 124/128 [13:20<00:24,  6.06s/it, pg=-0.275, rm=1.34, ret=1.34, glen=355, tlen=474, kl=0, cri=0.0367, vals=1.25][A
Train epoch [1/1]:  97%|█████████▋| 124/128 [13:26<00:24,  6.06s/it, pg=0.169, rm=0.559, ret=0.559, glen=294, tlen=460, kl=0, cri=0.0711, vals=0.78][A
Train epoch [1/1]:  98%|█████████▊| 125/128 [13:26<00:18,  6.04s/it, pg=0.169, rm=0.559, ret=0.559, glen=294, tlen=460, kl=0, cri=0.0711, vals=0.78][A
Train epoch [1/1]:  98%|█████████▊| 125/128 [13:32<00:18,  6.04s/it, pg=0.108, rm=0.821, ret=0.821, glen=443, tlen=602, kl=0, cri=0.0668, vals=1.17][A
Train epoch [1/1]:  98%|█████████▊| 126/128 [13:32<00:12,  6.12s/it, pg=0.108, rm=0.821, ret=0.821, glen=443, tlen=602, kl=0, cri=0.0668, vals=1.17][A
Train epoch [1/1]:  98%|█████████▊| 126/128 [13:38<00:12,  6.12s/it, pg=-0.176, rm=1.11, ret=1.11, glen=262, tlen=370, kl=0, cri=0.0534, vals=0.974][A
Train epoch [1/1]:  99%|█████████▉| 127/128 [13:38<00:06,  6.03s/it, pg=-0.176, rm=1.11, ret=1.11, glen=262, tlen=370, kl=0, cri=0.0534, vals=0.974][A
Train epoch [1/1]:  99%|█████████▉| 127/128 [13:51<00:06,  6.03s/it, pg=0.0299, rm=0.498, ret=0.498, glen=183, tlen=382, kl=0, cri=0.0774, vals=0.714][A
Train epoch [1/1]: 100%|██████████| 128/128 [13:51<00:00,  8.23s/it, pg=0.0299, rm=0.498, ret=0.498, glen=183, tlen=382, kl=0, cri=0.0774, vals=0.714][ATrain epoch [1/1]: 100%|██████████| 128/128 [13:51<00:00,  6.50s/it, pg=0.0299, rm=0.498, ret=0.498, glen=183, tlen=382, kl=0, cri=0.0774, vals=0.714]
Episode [1/1]:   2%|▏         | 63/3742 [52:46<39:21:47, 38.52s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 64/3742 [52:46<307:29:40, 300.97s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 65/3742 [53:24<226:53:44, 222.14s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 66/3742 [54:01<170:10:10, 166.65s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 67/3742 [54:38<130:25:37, 127.77s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 68/3742 [55:15<102:35:32, 100.53s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 69/3742 [55:54<83:50:29, 82.18s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]  Episode [1/1]:   2%|▏         | 70/3742 [56:27<68:36:29, 67.26s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
Episode [1/1]:   2%|▏         | 71/3742 [57:00<58:10:47, 57.05s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 72/3742 [57:39<52:28:28, 51.47s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 73/3742 [58:18<48:46:39, 47.86s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 74/3742 [58:56<45:51:25, 45.01s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 75/3742 [59:34<43:35:28, 42.79s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 76/3742 [59:48<34:36:44, 33.99s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 77/3742 [1:00:27<36:21:35, 35.72s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 78/3742 [1:00:58<34:45:44, 34.16s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 79/3742 [1:01:36<36:01:13, 35.40s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 80/3742 [1:01:49<29:08:45, 28.65s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 81/3742 [1:02:27<31:55:36, 31.39s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 82/3742 [1:03:05<33:53:58, 33.34s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 83/3742 [1:03:43<35:22:00, 34.80s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 84/3742 [1:04:22<36:41:07, 36.10s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 85/3742 [1:04:57<36:23:28, 35.82s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 86/3742 [1:05:36<37:12:51, 36.64s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 87/3742 [1:06:12<37:12:08, 36.64s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 88/3742 [1:06:51<37:40:50, 37.12s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 89/3742 [1:07:28<37:53:26, 37.34s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 90/3742 [1:08:05<37:40:53, 37.15s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 91/3742 [1:08:43<37:56:03, 37.40s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 92/3742 [1:09:22<38:17:02, 37.76s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   2%|▏         | 93/3742 [1:09:56<37:08:02, 36.64s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 94/3742 [1:10:34<37:34:26, 37.08s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 95/3742 [1:11:13<38:08:12, 37.65s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 96/3742 [1:11:50<37:58:30, 37.50s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 97/3742 [1:12:29<38:27:47, 37.99s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 98/3742 [1:13:05<37:57:43, 37.50s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 99/3742 [1:13:45<38:31:51, 38.08s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 100/3742 [1:14:24<38:57:28, 38.51s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 101/3742 [1:15:02<38:32:11, 38.10s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 102/3742 [1:15:39<38:23:14, 37.97s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 103/3742 [1:16:19<38:48:41, 38.40s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 104/3742 [1:16:58<39:05:42, 38.69s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 105/3742 [1:17:37<39:16:40, 38.88s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 106/3742 [1:18:13<38:20:08, 37.96s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
Episode [1/1]:   3%|▎         | 107/3742 [1:18:53<38:45:44, 38.39s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 108/3742 [1:19:32<39:03:12, 38.69s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 109/3742 [1:20:10<38:48:54, 38.46s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 110/3742 [1:20:49<39:06:47, 38.77s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 111/3742 [1:21:26<38:27:59, 38.14s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 112/3742 [1:22:05<38:36:44, 38.29s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 113/3742 [1:22:30<34:42:01, 34.42s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 114/3742 [1:23:07<35:29:08, 35.21s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 115/3742 [1:23:44<36:01:10, 35.75s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 116/3742 [1:24:22<36:33:20, 36.29s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 117/3742 [1:24:56<35:49:57, 35.59s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 118/3742 [1:25:33<36:29:58, 36.26s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 119/3742 [1:26:06<35:20:19, 35.11s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 120/3742 [1:26:38<34:32:13, 34.33s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 121/3742 [1:27:17<35:40:56, 35.48s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 122/3742 [1:27:56<36:51:08, 36.65s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 123/3742 [1:28:33<37:07:34, 36.93s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 124/3742 [1:29:12<37:32:08, 37.35s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 125/3742 [1:29:52<38:14:15, 38.06s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 126/3742 [1:30:29<37:57:24, 37.79s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]Episode [1/1]:   3%|▎         | 127/3742 [1:31:09<38:37:38, 38.47s/it, policy_loss=-0.0175, kl=0, reward=0.818, return=0.818, response_length=300, total_length=437, critic_loss=0.0425, values=0.718]user

Online application form The online application form is near on identical to last year, so if you applied last year, you know what to expect. I know it seems daunting but if you really believe this is the career for you and you are passionate about it, it is possible.
Internship thesis statement essay on mobile phone communication classification essay about friendship contoh narrative essay spm good creative writing openers short essay on benefits of exercise.
What can I expect at my interview? How much academic input will there be included in the programme? You might also want to check out the psychometric test info on our website, including practice test materials.
How much travel is involved in the STP? Read on for our interview with Heidi Kuoppama, during her first year on the Genetics Stream. What is the recruitment process? Yes they are; please refer to the in service information about the programme.
Due to the extensive variation in degrees available, it is not possible to provide a definitive list of relevant degrees for entry to the STP. You are required to have obtained your degree prior to the start date of the training programme.
Yes you will be required to attend an interview.
The interviewers do not have a copy of your application, therefore you are all on a level playing field, the trick is to take the experience you do have and interpret it in a way that becomes relevant and beneficial. What is the shortlisting process? The job can also be stressful at times. In my year there was a general science stand, research paper on reggae music specialism-specific stands so for me that was geneticsand then a stand on leadership, healthcare science and the NHS in general.
Once you start your final test you cannot hardcover thesis butterworth part way through or go back to the beginning. Some of the resources mentioned in this post may only be accessible by University of Manchester students.
Give details of your motivation, suitability and future hardcover thesis butterworth development or aspirations.
Shortlisting will be carried out on a national basis by appropriately qualified professional panels who will be representative of employers, professional bodies and HEIs delivering the MSc programmes. No there is not an age limit for the STP. To register on Oriel please click on account registration. Yes I was lucky enough to get my first choice, which was the lab I already nhs stp essay questions in.
Always take time to proof read your answers before you submit them a good tip is to read them backwards to spot typos. How many posts can I apply to? It will be a 3-hour multiple choice based test, which will be held on the same day as your interview.
How does the application process work? Once you have completed the registration process, you will need to login to the Oriel system to submit your application. You will have the opportunity to upload up to three supporting documents to your application.
Both papers consist of several multi-choice questions and a series of drag and drop exercises, as well as a free text section. For research paper on reggae music sample questions, please see the "Assessment" tab on the website.

Train epoch [1/1]:   0%|          | 0/128 [00:00<?, ?it/s][A
Train epoch [1/1]:   0%|          | 0/128 [00:04<?, ?it/s, pg=0.663, rm=0.427, ret=0.427, glen=253, tlen=403, kl=-8.85e-6, cri=0.0442, vals=0.673][A
Train epoch [1/1]:   1%|          | 1/128 [00:04<10:11,  4.81s/it, pg=0.663, rm=0.427, ret=0.427, glen=253, tlen=403, kl=-8.85e-6, cri=0.0442, vals=0.673][A
Train epoch [1/1]:   1%|          | 1/128 [00:10<10:11,  4.81s/it, pg=0.136, rm=0.994, ret=0.995, glen=222, tlen=379, kl=-0.000837, cri=0.019, vals=1.06] [A
Train epoch [1/1]:   2%|▏         | 2/128 [00:10<11:20,  5.40s/it, pg=0.136, rm=0.994, ret=0.995, glen=222, tlen=379, kl=-0.000837, cri=0.019, vals=1.06][A
Train epoch [1/1]:   2%|▏         | 2/128 [00:16<11:20,  5.40s/it, pg=0.0849, rm=0.767, ret=0.77, glen=318, tlen=502, kl=-0.00119, cri=0.0208, vals=0.909][A
Train epoch [1/1]:   2%|▏         | 3/128 [00:16<12:07,  5.82s/it, pg=0.0849, rm=0.767, ret=0.77, glen=318, tlen=502, kl=-0.00119, cri=0.0208, vals=0.909][A
Train epoch [1/1]:   2%|▏         | 3/128 [00:22<12:07,  5.82s/it, pg=0.839, rm=0.401, ret=0.401, glen=124, tlen=341, kl=-0.000197, cri=0.0523, vals=0.802][A
Train epoch [1/1]:   3%|▎         | 4/128 [00:22<11:58,  5.79s/it, pg=0.839, rm=0.401, ret=0.401, glen=124, tlen=341, kl=-0.000197, cri=0.0523, vals=0.802][A
Train epoch [1/1]:   3%|▎         | 4/128 [00:28<11:58,  5.79s/it, pg=0.0106, rm=0.582, ret=0.578, glen=309, tlen=579, kl=0.00122, cri=0.0272, vals=0.756] [A
Train epoch [1/1]:   4%|▍         | 5/128 [00:28<12:12,  5.96s/it, pg=0.0106, rm=0.582, ret=0.578, glen=309, tlen=579, kl=0.00122, cri=0.0272, vals=0.756][A
Train epoch [1/1]:   4%|▍         | 5/128 [00:34<12:12,  5.96s/it, pg=0.244, rm=0.726, ret=0.726, glen=159, tlen=323, kl=0.000339, cri=0.0282, vals=0.82] [A
Train epoch [1/1]:   5%|▍         | 6/128 [00:34<11:57,  5.88s/it, pg=0.244, rm=0.726, ret=0.726, glen=159, tlen=323, kl=0.000339, cri=0.0282, vals=0.82][A
Train epoch [1/1]:   5%|▍         | 6/128 [00:40<11:57,  5.88s/it, pg=1.14, rm=0.459, ret=0.459, glen=254, tlen=364, kl=-7.46e-5, cri=0.154, vals=0.795] [A
Train epoch [1/1]:   5%|▌         | 7/128 [00:40<11:50,  5.87s/it, pg=1.14, rm=0.459, ret=0.459, glen=254, tlen=364, kl=-7.46e-5, cri=0.154, vals=0.795][A
Train epoch [1/1]:   5%|▌         | 7/128 [00:46<11:50,  5.87s/it, pg=0.122, rm=0.924, ret=0.926, glen=261, tlen=307, kl=-0.000845, cri=0.0199, vals=1.04][A
Train epoch [1/1]:   6%|▋         | 8/128 [00:46<11:44,  5.87s/it, pg=0.122, rm=0.924, ret=0.926, glen=261, tlen=307, kl=-0.000845, cri=0.0199, vals=1.04][A
Train epoch [1/1]:   6%|▋         | 8/128 [00:52<11:44,  5.87s/it, pg=-0.0594, rm=0.88, ret=0.879, glen=384, tlen=456, kl=0.000345, cri=0.0231, vals=0.902][A
Train epoch [1/1]:   7%|▋         | 9/128 [00:52<11:44,  5.92s/it, pg=-0.0594, rm=0.88, ret=0.879, glen=384, tlen=456, kl=0.000345, cri=0.0231, vals=0.902][A
Train epoch [1/1]:   7%|▋         | 9/128 [00:58<11:44,  5.92s/it, pg=0.0268, rm=0.882, ret=0.882, glen=352, tlen=528, kl=1.86e-5, cri=0.0165, vals=0.866] [A
Train epoch [1/1]:   8%|▊         | 10/128 [00:58<11:40,  5.94s/it, pg=0.0268, rm=0.882, ret=0.882, glen=352, tlen=528, kl=1.86e-5, cri=0.0165, vals=0.866][A
Train epoch [1/1]:   8%|▊         | 10/128 [01:04<11:40,  5.94s/it, pg=0.417, rm=0.376, ret=0.377, glen=301, tlen=459, kl=-0.000348, cri=0.0265, vals=0.77][A
Train epoch [1/1]:   9%|▊         | 11/128 [01:04<11:41,  5.99s/it, pg=0.417, rm=0.376, ret=0.377, glen=301, tlen=459, kl=-0.000348, cri=0.0265, vals=0.77][A
Train epoch [1/1]:   9%|▊         | 11/128 [01:10<11:41,  5.99s/it, pg=0.191, rm=0.638, ret=0.638, glen=257, tlen=318, kl=0.000129, cri=0.0296, vals=0.582][A
Train epoch [1/1]:   9%|▉         | 12/128 [01:10<11:34,  5.99s/it, pg=0.191, rm=0.638, ret=0.638, glen=257, tlen=318, kl=0.000129, cri=0.0296, vals=0.582][A
Train epoch [1/1]:   9%|▉         | 12/128 [01:16<11:34,  5.99s/it, pg=0.0441, rm=0.927, ret=0.927, glen=404, tlen=562, kl=0.000125, cri=0.0153, vals=1.03][A
Train epoch [1/1]:  10%|█         | 13/128 [01:16<11:41,  6.10s/it, pg=0.0441, rm=0.927, ret=0.927, glen=404, tlen=562, kl=0.000125, cri=0.0153, vals=1.03][A
Train epoch [1/1]:  10%|█         | 13/128 [01:22<11:41,  6.10s/it, pg=0.287, rm=1.08, ret=1.08, glen=257, tlen=394, kl=-0.000838, cri=0.0295, vals=1.06]  [A
Train epoch [1/1]:  11%|█         | 14/128 [01:22<11:27,  6.03s/it, pg=0.287, rm=1.08, ret=1.08, glen=257, tlen=394, kl=-0.000838, cri=0.0295, vals=1.06][A
Train epoch [1/1]:  11%|█         | 14/128 [01:28<11:27,  6.03s/it, pg=0.193, rm=0.667, ret=0.666, glen=233, tlen=391, kl=0.00055, cri=0.0287, vals=0.886][A
Train epoch [1/1]:  12%|█▏        | 15/128 [01:28<11:23,  6.05s/it, pg=0.193, rm=0.667, ret=0.666, glen=233, tlen=391, kl=0.00055, cri=0.0287, vals=0.886][A
Train epoch [1/1]:  12%|█▏        | 15/128 [01:42<11:23,  6.05s/it, pg=0.0131, rm=0.999, ret=0.998, glen=365, tlen=529, kl=0.000259, cri=0.0173, vals=1.01][A
Train epoch [1/1]:  12%|█▎        | 16/128 [01:42<15:21,  8.23s/it, pg=0.0131, rm=0.999, ret=0.998, glen=365, tlen=529, kl=0.000259, cri=0.0173, vals=1.01][A
Train epoch [1/1]:  12%|█▎        | 16/128 [01:47<15:21,  8.23s/it, pg=0.718, rm=0.468, ret=0.468, glen=209, tlen=384, kl=-3.87e-5, cri=0.0318, vals=0.553][A
Train epoch [1/1]:  13%|█▎        | 17/128 [01:47<13:22,  7.23s/it, pg=0.718, rm=0.468, ret=0.468, glen=209, tlen=384, kl=-3.87e-5, cri=0.0318, vals=0.553][A
Train epoch [1/1]:  13%|█▎        | 17/128 [01:52<13:22,  7.23s/it, pg=0.88, rm=0.366, ret=0.368, glen=256, tlen=419, kl=-0.000974, cri=0.0477, vals=0.655][A
Train epoch [1/1]:  14%|█▍        | 18/128 [01:52<12:28,  6.81s/it, pg=0.88, rm=0.366, ret=0.368, glen=256, tlen=419, kl=-0.000974, cri=0.0477, vals=0.655][A
Train epoch [1/1]:  14%|█▍        | 18/128 [01:59<12:28,  6.81s/it, pg=0.826, rm=0.768, ret=0.767, glen=293, tlen=502, kl=7.83e-5, cri=0.0464, vals=1.11]  [A
Train epoch [1/1]:  15%|█▍        | 19/128 [01:59<12:03,  6.64s/it, pg=0.826, rm=0.768, ret=0.767, glen=293, tlen=502, kl=7.83e-5, cri=0.0464, vals=1.11][A
Train epoch [1/1]:  15%|█▍        | 19/128 [02:05<12:03,  6.64s/it, pg=0.0507, rm=0.946, ret=0.945, glen=382, tlen=515, kl=0.000333, cri=0.0165, vals=1.04][A
Train epoch [1/1]:  16%|█▌        | 20/128 [02:05<11:35,  6.44s/it, pg=0.0507, rm=0.946, ret=0.945, glen=382, tlen=515, kl=0.000333, cri=0.0165, vals=1.04][A
Train epoch [1/1]:  16%|█▌        | 20/128 [02:10<11:35,  6.44s/it, pg=0.0868, rm=0.52, ret=0.52, glen=308, tlen=392, kl=-4.05e-5, cri=0.0231, vals=0.769] [A
Train epoch [1/1]:  16%|█▋        | 21/128 [02:10<11:11,  6.28s/it, pg=0.0868, rm=0.52, ret=0.52, glen=308, tlen=392, kl=-4.05e-5, cri=0.0231, vals=0.769][A
Train epoch [1/1]:  16%|█▋        | 21/128 [02:16<11:11,  6.28s/it, pg=-0.117, rm=0.908, ret=0.91, glen=202, tlen=337, kl=-0.00118, cri=0.0331, vals=0.751][A
Train epoch [1/1]:  17%|█▋        | 22/128 [02:16<10:51,  6.15s/it, pg=-0.117, rm=0.908, ret=0.91, glen=202, tlen=337, kl=-0.00118, cri=0.0331, vals=0.751][A
Train epoch [1/1]:  17%|█▋        | 22/128 [02:22<10:51,  6.15s/it, pg=-0.167, rm=1.22, ret=1.22, glen=319, tlen=406, kl=0.00073, cri=0.0228, vals=0.987]  [A
Train epoch [1/1]:  18%|█▊        | 23/128 [02:22<10:38,  6.08s/it, pg=-0.167, rm=1.22, ret=1.22, glen=319, tlen=406, kl=0.00073, cri=0.0228, vals=0.987][A
Train epoch [1/1]:  18%|█▊        | 23/128 [02:28<10:38,  6.08s/it, pg=0.695, rm=0.321, ret=0.323, glen=197, tlen=379, kl=-0.000899, cri=0.0386, vals=0.549][A
Train epoch [1/1]:  19%|█▉        | 24/128 [02:28<10:32,  6.08s/it, pg=0.695, rm=0.321, ret=0.323, glen=197, tlen=379, kl=-0.000899, cri=0.0386, vals=0.549][A
Train epoch [1/1]:  19%|█▉        | 24/128 [02:34<10:32,  6.08s/it, pg=0.392, rm=0.853, ret=0.853, glen=304, tlen=386, kl=-0.00011, cri=0.0473, vals=0.956] [A
Train epoch [1/1]:  20%|█▉        | 25/128 [02:34<10:27,  6.09s/it, pg=0.392, rm=0.853, ret=0.853, glen=304, tlen=386, kl=-0.00011, cri=0.0473, vals=0.956][A
Train epoch [1/1]:  20%|█▉        | 25/128 [02:40<10:27,  6.09s/it, pg=0.0804, rm=1.16, ret=1.15, glen=323, tlen=394, kl=0.000415, cri=0.0209, vals=0.969] [A
Train epoch [1/1]:  20%|██        | 26/128 [02:40<10:17,  6.05s/it, pg=0.0804, rm=1.16, ret=1.15, glen=323, tlen=394, kl=0.000415, cri=0.0209, vals=0.969][A
Train epoch [1/1]:  20%|██        | 26/128 [02:46<10:17,  6.05s/it, pg=0.0983, rm=0.74, ret=0.738, glen=447, tlen=506, kl=0.000483, cri=0.0131, vals=0.783][A
Train epoch [1/1]:  21%|██        | 27/128 [02:46<10:07,  6.01s/it, pg=0.0983, rm=0.74, ret=0.738, glen=447, tlen=506, kl=0.000483, cri=0.0131, vals=0.783][A
Train epoch [1/1]:  21%|██        | 27/128 [02:52<10:07,  6.01s/it, pg=-0.0168, rm=0.778, ret=0.777, glen=234, tlen=381, kl=0.000325, cri=0.0176, vals=0.755][A
Train epoch [1/1]:  22%|██▏       | 28/128 [02:52<09:55,  5.96s/it, pg=-0.0168, rm=0.778, ret=0.777, glen=234, tlen=381, kl=0.000325, cri=0.0176, vals=0.755][A
Train epoch [1/1]:  22%|██▏       | 28/128 [02:58<09:55,  5.96s/it, pg=0.0185, rm=0.726, ret=0.723, glen=391, tlen=440, kl=0.000732, cri=0.0196, vals=0.828] [A
Train epoch [1/1]:  23%|██▎       | 29/128 [02:58<09:47,  5.94s/it, pg=0.0185, rm=0.726, ret=0.723, glen=391, tlen=440, kl=0.000732, cri=0.0196, vals=0.828][A
Train epoch [1/1]:  23%|██▎       | 29/128 [03:04<09:47,  5.94s/it, pg=-0.149, rm=1.48, ret=1.48, glen=375, tlen=502, kl=0.000631, cri=0.0209, vals=1.21]   [A
Train epoch [1/1]:  23%|██▎       | 30/128 [03:04<09:43,  5.95s/it, pg=-0.149, rm=1.48, ret=1.48, glen=375, tlen=502, kl=0.000631, cri=0.0209, vals=1.21][A
Train epoch [1/1]:  23%|██▎       | 30/128 [03:10<09:43,  5.95s/it, pg=0.675, rm=0.698, ret=0.7, glen=314, tlen=522, kl=-0.000819, cri=0.065, vals=0.798][A
Train epoch [1/1]:  24%|██▍       | 31/128 [03:10<09:39,  5.98s/it, pg=0.675, rm=0.698, ret=0.7, glen=314, tlen=522, kl=-0.000819, cri=0.065, vals=0.798][A
Train epoch [1/1]:  24%|██▍       | 31/128 [03:23<09:39,  5.98s/it, pg=0.00858, rm=0.741, ret=0.742, glen=356, tlen=521, kl=-0.000295, cri=0.0209, vals=0.894][A
Train epoch [1/1]:  25%|██▌       | 32/128 [03:23<13:09,  8.22s/it, pg=0.00858, rm=0.741, ret=0.742, glen=356, tlen=521, kl=-0.000295, cri=0.0209, vals=0.894][A
Train epoch [1/1]:  25%|██▌       | 32/128 [03:28<13:09,  8.22s/it, pg=0.443, rm=0.486, ret=0.486, glen=287, tlen=370, kl=1.64e-5, cri=0.0238, vals=0.646]    [A
Train epoch [1/1]:  26%|██▌       | 33/128 [03:28<11:24,  7.20s/it, pg=0.443, rm=0.486, ret=0.486, glen=287, tlen=370, kl=1.64e-5, cri=0.0238, vals=0.646][A
Train epoch [1/1]:  26%|██▌       | 33/128 [03:34<11:24,  7.20s/it, pg=0.309, rm=0.654, ret=0.653, glen=279, tlen=400, kl=0.000292, cri=0.0255, vals=0.718][A
Train epoch [1/1]:  27%|██▋       | 34/128 [03:34<10:42,  6.84s/it, pg=0.309, rm=0.654, ret=0.653, glen=279, tlen=400, kl=0.000292, cri=0.0255, vals=0.718][A
Train epoch [1/1]:  27%|██▋       | 34/128 [03:40<10:42,  6.84s/it, pg=-0.0068, rm=0.757, ret=0.757, glen=347, tlen=506, kl=0.000235, cri=0.0178, vals=0.755][A
Train epoch [1/1]:  27%|██▋       | 35/128 [03:40<10:17,  6.64s/it, pg=-0.0068, rm=0.757, ret=0.757, glen=347, tlen=506, kl=0.000235, cri=0.0178, vals=0.755][A
Train epoch [1/1]:  27%|██▋       | 35/128 [03:46<10:17,  6.64s/it, pg=0.0948, rm=0.93, ret=0.934, glen=300, tlen=409, kl=-0.00116, cri=0.0228, vals=0.756]  [A
Train epoch [1/1]:  28%|██▊       | 36/128 [03:46<09:48,  6.40s/it, pg=0.0948, rm=0.93, ret=0.934, glen=300, tlen=409, kl=-0.00116, cri=0.0228, vals=0.756][A
Train epoch [1/1]:  28%|██▊       | 36/128 [03:52<09:48,  6.40s/it, pg=-0.275, rm=1.18, ret=1.18, glen=257, tlen=338, kl=0.000356, cri=0.0233, vals=0.884] [A
Train epoch [1/1]:  29%|██▉       | 37/128 [03:52<09:31,  6.28s/it, pg=-0.275, rm=1.18, ret=1.18, glen=257, tlen=338, kl=0.000356, cri=0.0233, vals=0.884][A
Train epoch [1/1]:  29%|██▉       | 37/128 [03:58<09:31,  6.28s/it, pg=0.206, rm=0.806, ret=0.807, glen=262, tlen=366, kl=-0.000433, cri=0.0255, vals=0.87][A
Train epoch [1/1]:  30%|██▉       | 38/128 [03:58<09:10,  6.12s/it, pg=0.206, rm=0.806, ret=0.807, glen=262, tlen=366, kl=-0.000433, cri=0.0255, vals=0.87][A
Train epoch [1/1]:  30%|██▉       | 38/128 [04:04<09:10,  6.12s/it, pg=0.396, rm=0.747, ret=0.745, glen=266, tlen=446, kl=0.000542, cri=0.026, vals=0.894] [A
Train epoch [1/1]:  30%|███       | 39/128 [04:04<08:59,  6.06s/it, pg=0.396, rm=0.747, ret=0.745, glen=266, tlen=446, kl=0.000542, cri=0.026, vals=0.894][A
Train epoch [1/1]:  30%|███       | 39/128 [04:10<08:59,  6.06s/it, pg=0.579, rm=0.583, ret=0.58, glen=294, tlen=374, kl=0.00095, cri=0.0342, vals=0.828] [A
Train epoch [1/1]:  31%|███▏      | 40/128 [04:10<08:48,  6.01s/it, pg=0.579, rm=0.583, ret=0.58, glen=294, tlen=374, kl=0.00095, cri=0.0342, vals=0.828][A
Train epoch [1/1]:  31%|███▏      | 40/128 [04:16<08:48,  6.01s/it, pg=-0.325, rm=0.602, ret=0.604, glen=214, tlen=319, kl=-0.00101, cri=0.0354, vals=0.595][A
Train epoch [1/1]:  32%|███▏      | 41/128 [04:16<08:36,  5.94s/it, pg=-0.325, rm=0.602, ret=0.604, glen=214, tlen=319, kl=-0.00101, cri=0.0354, vals=0.595][A
Train epoch [1/1]:  32%|███▏      | 41/128 [04:22<08:36,  5.94s/it, pg=-0.125, rm=0.784, ret=0.785, glen=354, tlen=473, kl=-0.0003, cri=0.0162, vals=1.03]  [A
Train epoch [1/1]:  33%|███▎      | 42/128 [04:22<08:33,  5.97s/it, pg=-0.125, rm=0.784, ret=0.785, glen=354, tlen=473, kl=-0.0003, cri=0.0162, vals=1.03][A
Train epoch [1/1]:  33%|███▎      | 42/128 [04:28<08:33,  5.97s/it, pg=-0.0501, rm=0.72, ret=0.721, glen=290, tlen=428, kl=-0.000437, cri=0.0272, vals=0.882][A
Train epoch [1/1]:  34%|███▎      | 43/128 [04:28<08:31,  6.02s/it, pg=-0.0501, rm=0.72, ret=0.721, glen=290, tlen=428, kl=-0.000437, cri=0.0272, vals=0.882][A
Train epoch [1/1]:  34%|███▎      | 43/128 [04:34<08:31,  6.02s/it, pg=0.055, rm=0.829, ret=0.829, glen=310, tlen=370, kl=0.000102, cri=0.0165, vals=1.03]   [A
Train epoch [1/1]:  34%|███▍      | 44/128 [04:34<08:25,  6.02s/it, pg=0.055, rm=0.829, ret=0.829, glen=310, tlen=370, kl=0.000102, cri=0.0165, vals=1.03][A
Train epoch [1/1]:  34%|███▍      | 44/128 [04:40<08:25,  6.02s/it, pg=-0.171, rm=1.19, ret=1.19, glen=347, tlen=427, kl=0.000134, cri=0.0152, vals=1.04] [A
Train epoch [1/1]:  35%|███▌      | 45/128 [04:40<08:15,  5.98s/it, pg=-0.171, rm=1.19, ret=1.19, glen=347, tlen=427, kl=0.000134, cri=0.0152, vals=1.04][A
Train epoch [1/1]:  35%|███▌      | 45/128 [04:46<08:15,  5.98s/it, pg=0.153, rm=0.969, ret=0.966, glen=388, tlen=448, kl=0.000685, cri=0.0349, vals=0.957][A
Train epoch [1/1]:  36%|███▌      | 46/128 [04:46<08:14,  6.03s/it, pg=0.153, rm=0.969, ret=0.966, glen=388, tlen=448, kl=0.000685, cri=0.0349, vals=0.957][A
Train epoch [1/1]:  36%|███▌      | 46/128 [04:52<08:14,  6.03s/it, pg=-0.00519, rm=1.12, ret=1.12, glen=312, tlen=423, kl=0.000271, cri=0.0187, vals=1.03][A
Train epoch [1/1]:  37%|███▋      | 47/128 [04:52<08:07,  6.01s/it, pg=-0.00519, rm=1.12, ret=1.12, glen=312, tlen=423, kl=0.000271, cri=0.0187, vals=1.03][A
Train epoch [1/1]:  37%|███▋      | 47/128 [05:05<08:07,  6.01s/it, pg=0.276, rm=0.511, ret=0.513, glen=396, tlen=452, kl=-0.000496, cri=0.027, vals=0.885][A
Train epoch [1/1]:  38%|███▊      | 48/128 [05:05<10:57,  8.22s/it, pg=0.276, rm=0.511, ret=0.513, glen=396, tlen=452, kl=-0.000496, cri=0.027, vals=0.885][A
Train epoch [1/1]:  38%|███▊      | 48/128 [05:10<10:57,  8.22s/it, pg=0.355, rm=0.734, ret=0.731, glen=210, tlen=380, kl=0.00119, cri=0.0597, vals=0.872] [A
Train epoch [1/1]:  38%|███▊      | 49/128 [05:10<09:24,  7.15s/it, pg=0.355, rm=0.734, ret=0.731, glen=210, tlen=380, kl=0.00119, cri=0.0597, vals=0.872][A
Train epoch [1/1]:  38%|███▊      | 49/128 [05:16<09:24,  7.15s/it, pg=0.31, rm=0.511, ret=0.515, glen=216, tlen=377, kl=-0.00153, cri=0.0474, vals=0.712][A
Train epoch [1/1]:  39%|███▉      | 50/128 [05:16<08:46,  6.75s/it, pg=0.31, rm=0.511, ret=0.515, glen=216, tlen=377, kl=-0.00153, cri=0.0474, vals=0.712][A
Train epoch [1/1]:  39%|███▉      | 50/128 [05:22<08:46,  6.75s/it, pg=1.14, rm=0.442, ret=0.442, glen=200, tlen=424, kl=-0.000217, cri=0.0905, vals=0.681][A
Train epoch [1/1]:  40%|███▉      | 51/128 [05:22<08:20,  6.50s/it, pg=1.14, rm=0.442, ret=0.442, glen=200, tlen=424, kl=-0.000217, cri=0.0905, vals=0.681][A
Train epoch [1/1]:  40%|███▉      | 51/128 [05:28<08:20,  6.50s/it, pg=0.586, rm=0.518, ret=0.519, glen=290, tlen=471, kl=-0.000313, cri=0.0338, vals=1.12][A
Train epoch [1/1]:  41%|████      | 52/128 [05:28<08:01,  6.33s/it, pg=0.586, rm=0.518, ret=0.519, glen=290, tlen=471, kl=-0.000313, cri=0.0338, vals=1.12][A
Train epoch [1/1]:  41%|████      | 52/128 [05:34<08:01,  6.33s/it, pg=-0.183, rm=1.17, ret=1.17, glen=377, tlen=432, kl=-0.000392, cri=0.0114, vals=1.21] [A
Train epoch [1/1]:  41%|████▏     | 53/128 [05:34<07:46,  6.22s/it, pg=-0.183, rm=1.17, ret=1.17, glen=377, tlen=432, kl=-0.000392, cri=0.0114, vals=1.21][A
Train epoch [1/1]:  41%|████▏     | 53/128 [05:39<07:46,  6.22s/it, pg=0.369, rm=1.3, ret=1.3, glen=287, tlen=462, kl=-0.00106, cri=0.0406, vals=1.14]    [A
Train epoch [1/1]:  42%|████▏     | 54/128 [05:39<07:35,  6.15s/it, pg=0.369, rm=1.3, ret=1.3, glen=287, tlen=462, kl=-0.00106, cri=0.0406, vals=1.14][A
Train epoch [1/1]:  42%|████▏     | 54/128 [05:46<07:35,  6.15s/it, pg=-0.0351, rm=0.876, ret=0.877, glen=337, tlen=604, kl=-6.15e-5, cri=0.0138, vals=0.869][A
Train epoch [1/1]:  43%|████▎     | 55/128 [05:46<07:33,  6.21s/it, pg=-0.0351, rm=0.876, ret=0.877, glen=337, tlen=604, kl=-6.15e-5, cri=0.0138, vals=0.869][A
Train epoch [1/1]:  43%|████▎     | 55/128 [05:52<07:33,  6.21s/it, pg=0.0847, rm=0.976, ret=0.978, glen=314, tlen=419, kl=-0.000777, cri=0.0172, vals=0.867][A
Train epoch [1/1]:  44%|████▍     | 56/128 [05:52<07:18,  6.10s/it, pg=0.0847, rm=0.976, ret=0.978, glen=314, tlen=419, kl=-0.000777, cri=0.0172, vals=0.867][A
Train epoch [1/1]:  44%|████▍     | 56/128 [05:58<07:18,  6.10s/it, pg=0.259, rm=0.823, ret=0.826, glen=272, tlen=345, kl=-0.00107, cri=0.0318, vals=1.12]   [A
Train epoch [1/1]:  45%|████▍     | 57/128 [05:58<07:11,  6.07s/it, pg=0.259, rm=0.823, ret=0.826, glen=272, tlen=345, kl=-0.00107, cri=0.0318, vals=1.12][A
Train epoch [1/1]:  45%|████▍     | 57/128 [06:04<07:11,  6.07s/it, pg=0.807, rm=0.671, ret=0.673, glen=276, tlen=472, kl=-0.000649, cri=0.0468, vals=1.15][A
Train epoch [1/1]:  45%|████▌     | 58/128 [06:04<07:03,  6.04s/it, pg=0.807, rm=0.671, ret=0.673, glen=276, tlen=472, kl=-0.000649, cri=0.0468, vals=1.15][A
Train epoch [1/1]:  45%|████▌     | 58/128 [06:10<07:03,  6.04s/it, pg=0.138, rm=0.634, ret=0.634, glen=350, tlen=501, kl=8.41e-5, cri=0.0165, vals=0.853] [A
Train epoch [1/1]:  46%|████▌     | 59/128 [06:10<07:02,  6.12s/it, pg=0.138, rm=0.634, ret=0.634, glen=350, tlen=501, kl=8.41e-5, cri=0.0165, vals=0.853][A
Train epoch [1/1]:  46%|████▌     | 59/128 [06:16<07:02,  6.12s/it, pg=0.426, rm=0.58, ret=0.578, glen=306, tlen=477, kl=0.000615, cri=0.0754, vals=0.935][A
Train epoch [1/1]:  47%|████▋     | 60/128 [06:16<06:58,  6.16s/it, pg=0.426, rm=0.58, ret=0.578, glen=306, tlen=477, kl=0.000615, cri=0.0754, vals=0.935][A
Train epoch [1/1]:  47%|████▋     | 60/128 [06:22<06:58,  6.16s/it, pg=0.0956, rm=0.962, ret=0.964, glen=349, tlen=452, kl=-0.000427, cri=0.022, vals=1.1][A
Train epoch [1/1]:  48%|████▊     | 61/128 [06:22<06:48,  6.09s/it, pg=0.0956, rm=0.962, ret=0.964, glen=349, tlen=452, kl=-0.000427, cri=0.022, vals=1.1][A
Train epoch [1/1]:  48%|████▊     | 61/128 [06:28<06:48,  6.09s/it, pg=0.543, rm=0.677, ret=0.679, glen=299, tlen=405, kl=-0.000863, cri=0.0378, vals=1.06][A
Train epoch [1/1]:  48%|████▊     | 62/128 [06:28<06:42,  6.09s/it, pg=0.543, rm=0.677, ret=0.679, glen=299, tlen=405, kl=-0.000863, cri=0.0378, vals=1.06][A
Train epoch [1/1]:  48%|████▊     | 62/128 [06:34<06:42,  6.09s/it, pg=0.477, rm=1.1, ret=1.1, glen=367, tlen=485, kl=-0.000328, cri=0.0183, vals=1.27]    [A
Train epoch [1/1]:  49%|████▉     | 63/128 [06:34<06:32,  6.04s/it, pg=0.477, rm=1.1, ret=1.1, glen=367, tlen=485, kl=-0.000328, cri=0.0183, vals=1.27][A
Train epoch [1/1]:  49%|████▉     | 63/128 [06:47<06:32,  6.04s/it, pg=0.257, rm=0.213, ret=0.213, glen=143, tlen=401, kl=9.44e-5, cri=0.0416, vals=0.203][A
Train epoch [1/1]:  50%|█████     | 64/128 [06:47<08:42,  8.16s/it, pg=0.257, rm=0.213, ret=0.213, glen=143, tlen=401, kl=9.44e-5, cri=0.0416, vals=0.203][A
Train epoch [1/1]:  50%|█████     | 64/128 [06:52<08:42,  8.16s/it, pg=-0.0654, rm=0.825, ret=0.822, glen=310, tlen=388, kl=0.000899, cri=0.0269, vals=1.01][A
Train epoch [1/1]:  51%|█████     | 65/128 [06:52<07:27,  7.11s/it, pg=-0.0654, rm=0.825, ret=0.822, glen=310, tlen=388, kl=0.000899, cri=0.0269, vals=1.01][A
Train epoch [1/1]:  51%|█████     | 65/128 [06:58<07:27,  7.11s/it, pg=-0.119, rm=1.1, ret=1.1, glen=298, tlen=390, kl=-0.00035, cri=0.0314, vals=1.04]     [A
Train epoch [1/1]:  52%|█████▏    | 66/128 [06:58<06:58,  6.75s/it, pg=-0.119, rm=1.1, ret=1.1, glen=298, tlen=390, kl=-0.00035, cri=0.0314, vals=1.04][A
Train epoch [1/1]:  52%|█████▏    | 66/128 [07:04<06:58,  6.75s/it, pg=0.303, rm=0.533, ret=0.535, glen=226, tlen=437, kl=-0.000614, cri=0.0495, vals=1.07][A
Train epoch [1/1]:  52%|█████▏    | 67/128 [07:04<06:41,  6.59s/it, pg=0.303, rm=0.533, ret=0.535, glen=226, tlen=437, kl=-0.000614, cri=0.0495, vals=1.07][A
Train epoch [1/1]:  52%|█████▏    | 67/128 [07:10<06:41,  6.59s/it, pg=0.565, rm=0.764, ret=0.76, glen=330, tlen=458, kl=0.000965, cri=0.0421, vals=1.09]  [A
Train epoch [1/1]:  53%|█████▎    | 68/128 [07:10<06:26,  6.44s/it, pg=0.565, rm=0.764, ret=0.76, glen=330, tlen=458, kl=0.000965, cri=0.0421, vals=1.09][A
Train epoch [1/1]:  53%|█████▎    | 68/128 [07:16<06:26,  6.44s/it, pg=0.000559, rm=1.21, ret=1.21, glen=367, tlen=502, kl=-0.000261, cri=0.02, vals=1.17][A
Train epoch [1/1]:  54%|█████▍    | 69/128 [07:16<06:11,  6.29s/it, pg=0.000559, rm=1.21, ret=1.21, glen=367, tlen=502, kl=-0.000261, cri=0.02, vals=1.17][A
Train epoch [1/1]:  54%|█████▍    | 69/128 [07:22<06:11,  6.29s/it, pg=0.237, rm=0.488, ret=0.488, glen=256, tlen=494, kl=-3.19e-5, cri=0.0241, vals=1.01][A
Train epoch [1/1]:  55%|█████▍    | 70/128 [07:22<06:05,  6.30s/it, pg=0.237, rm=0.488, ret=0.488, glen=256, tlen=494, kl=-3.19e-5, cri=0.0241, vals=1.01][A
Train epoch [1/1]:  55%|█████▍    | 70/128 [07:28<06:05,  6.30s/it, pg=-0.0738, rm=0.89, ret=0.89, glen=294, tlen=409, kl=-4.05e-5, cri=0.021, vals=0.999][A
Train epoch [1/1]:  55%|█████▌    | 71/128 [07:28<05:52,  6.18s/it, pg=-0.0738, rm=0.89, ret=0.89, glen=294, tlen=409, kl=-4.05e-5, cri=0.021, vals=0.999][A
Train epoch [1/1]:  55%|█████▌    | 71/128 [07:34<05:52,  6.18s/it, pg=0.49, rm=0.288, ret=0.288, glen=202, tlen=446, kl=0.000189, cri=0.0303, vals=0.589][A
Train epoch [1/1]:  56%|█████▋    | 72/128 [07:34<05:44,  6.15s/it, pg=0.49, rm=0.288, ret=0.288, glen=202, tlen=446, kl=0.000189, cri=0.0303, vals=0.589][A
Train epoch [1/1]:  56%|█████▋    | 72/128 [07:40<05:44,  6.15s/it, pg=-0.0859, rm=1.15, ret=1.15, glen=330, tlen=395, kl=-0.000667, cri=0.0191, vals=1.23][A
Train epoch [1/1]:  57%|█████▋    | 73/128 [07:40<05:34,  6.08s/it, pg=-0.0859, rm=1.15, ret=1.15, glen=330, tlen=395, kl=-0.000667, cri=0.0191, vals=1.23][A
Train epoch [1/1]:  57%|█████▋    | 73/128 [07:46<05:34,  6.08s/it, pg=-0.0397, rm=1, ret=1, glen=244, tlen=418, kl=0.000173, cri=0.0196, vals=0.993]      [A
Train epoch [1/1]:  58%|█████▊    | 74/128 [07:46<05:24,  6.01s/it, pg=-0.0397, rm=1, ret=1, glen=244, tlen=418, kl=0.000173, cri=0.0196, vals=0.993][A
Train epoch [1/1]:  58%|█████▊    | 74/128 [07:52<05:24,  6.01s/it, pg=0.467, rm=0.688, ret=0.691, glen=322, tlen=503, kl=-0.00112, cri=0.0238, vals=1.17][A
Train epoch [1/1]:  59%|█████▊    | 75/128 [07:52<05:18,  6.02s/it, pg=0.467, rm=0.688, ret=0.691, glen=322, tlen=503, kl=-0.00112, cri=0.0238, vals=1.17][A
Train epoch [1/1]:  59%|█████▊    | 75/128 [07:58<05:18,  6.02s/it, pg=-0.182, rm=1.05, ret=1.05, glen=369, tlen=452, kl=-0.000247, cri=0.0502, vals=1.09][A
Train epoch [1/1]:  59%|█████▉    | 76/128 [07:58<05:11,  5.99s/it, pg=-0.182, rm=1.05, ret=1.05, glen=369, tlen=452, kl=-0.000247, cri=0.0502, vals=1.09][A
Train epoch [1/1]:  59%|█████▉    | 76/128 [08:04<05:11,  5.99s/it, pg=-0.0615, rm=0.967, ret=0.967, glen=298, tlen=463, kl=-0.000102, cri=0.0144, vals=1.19][A
Train epoch [1/1]:  60%|██████    | 77/128 [08:04<05:08,  6.05s/it, pg=-0.0615, rm=0.967, ret=0.967, glen=298, tlen=463, kl=-0.000102, cri=0.0144, vals=1.19][A
Train epoch [1/1]:  60%|██████    | 77/128 [08:10<05:08,  6.05s/it, pg=0.0425, rm=1.06, ret=1.05, glen=288, tlen=425, kl=0.00155, cri=0.0279, vals=1.1]      [A
Train epoch [1/1]:  61%|██████    | 78/128 [08:10<05:02,  6.06s/it, pg=0.0425, rm=1.06, ret=1.05, glen=288, tlen=425, kl=0.00155, cri=0.0279, vals=1.1][A
Train epoch [1/1]:  61%|██████    | 78/128 [08:16<05:02,  6.06s/it, pg=-0.0849, rm=0.932, ret=0.932, glen=357, tlen=420, kl=-3.2e-5, cri=0.0208, vals=1.11][A
Train epoch [1/1]:  62%|██████▏   | 79/128 [08:16<04:57,  6.07s/it, pg=-0.0849, rm=0.932, ret=0.932, glen=357, tlen=420, kl=-3.2e-5, cri=0.0208, vals=1.11][A
Train epoch [1/1]:  62%|██████▏   | 79/128 [08:29<04:57,  6.07s/it, pg=0.364, rm=0.64, ret=0.64, glen=194, tlen=259, kl=0.000171, cri=0.0596, vals=0.794]  [A
Train epoch [1/1]:  62%|██████▎   | 80/128 [08:29<06:31,  8.15s/it, pg=0.364, rm=0.64, ret=0.64, glen=194, tlen=259, kl=0.000171, cri=0.0596, vals=0.794][A
Train epoch [1/1]:  62%|██████▎   | 80/128 [08:35<06:31,  8.15s/it, pg=0.432, rm=1.1, ret=1.1, glen=337, tlen=455, kl=-1.57e-5, cri=0.0369, vals=1.2]    [A
Train epoch [1/1]:  63%|██████▎   | 81/128 [08:35<05:39,  7.22s/it, pg=0.432, rm=1.1, ret=1.1, glen=337, tlen=455, kl=-1.57e-5, cri=0.0369, vals=1.2][A
Train epoch [1/1]:  63%|██████▎   | 81/128 [08:41<05:39,  7.22s/it, pg=0.0023, rm=1.01, ret=1.01, glen=503, tlen=556, kl=-0.000543, cri=0.0141, vals=1.17][A
Train epoch [1/1]:  64%|██████▍   | 82/128 [08:41<05:15,  6.86s/it, pg=0.0023, rm=1.01, ret=1.01, glen=503, tlen=556, kl=-0.000543, cri=0.0141, vals=1.17][A
Train epoch [1/1]:  64%|██████▍   | 82/128 [08:47<05:15,  6.86s/it, pg=0.754, rm=0.259, ret=0.256, glen=243, tlen=556, kl=0.000968, cri=0.0416, vals=0.364][A
Train epoch [1/1]:  65%|██████▍   | 83/128 [08:47<05:01,  6.70s/it, pg=0.754, rm=0.259, ret=0.256, glen=243, tlen=556, kl=0.000968, cri=0.0416, vals=0.364][A
Train epoch [1/1]:  65%|██████▍   | 83/128 [08:53<05:01,  6.70s/it, pg=0.0807, rm=0.663, ret=0.664, glen=364, tlen=512, kl=-0.000233, cri=0.0202, vals=0.798][A
Train epoch [1/1]:  66%|██████▌   | 84/128 [08:53<04:47,  6.53s/it, pg=0.0807, rm=0.663, ret=0.664, glen=364, tlen=512, kl=-0.000233, cri=0.0202, vals=0.798][A
Train epoch [1/1]:  66%|██████▌   | 84/128 [08:59<04:47,  6.53s/it, pg=0.0973, rm=0.621, ret=0.621, glen=259, tlen=366, kl=-0.000294, cri=0.0226, vals=0.749][A
Train epoch [1/1]:  66%|██████▋   | 85/128 [08:59<04:31,  6.31s/it, pg=0.0973, rm=0.621, ret=0.621, glen=259, tlen=366, kl=-0.000294, cri=0.0226, vals=0.749][A
Train epoch [1/1]:  66%|██████▋   | 85/128 [09:05<04:31,  6.31s/it, pg=-0.0885, rm=1.09, ret=1.1, glen=335, tlen=452, kl=-0.000654, cri=0.0226, vals=1.02]   [A
Train epoch [1/1]:  67%|██████▋   | 86/128 [09:05<04:20,  6.21s/it, pg=-0.0885, rm=1.09, ret=1.1, glen=335, tlen=452, kl=-0.000654, cri=0.0226, vals=1.02][A
Train epoch [1/1]:  67%|██████▋   | 86/128 [09:11<04:20,  6.21s/it, pg=-0.132, rm=1.3, ret=1.3, glen=397, tlen=480, kl=-0.000478, cri=0.018, vals=1.15]   [A
Train epoch [1/1]:  68%|██████▊   | 87/128 [09:11<04:12,  6.15s/it, pg=-0.132, rm=1.3, ret=1.3, glen=397, tlen=480, kl=-0.000478, cri=0.018, vals=1.15][A
Train epoch [1/1]:  68%|██████▊   | 87/128 [09:17<04:12,  6.15s/it, pg=-0.028, rm=0.913, ret=0.917, glen=425, tlen=570, kl=-0.000985, cri=0.0173, vals=0.918][A
Train epoch [1/1]:  69%|██████▉   | 88/128 [09:17<04:07,  6.18s/it, pg=-0.028, rm=0.913, ret=0.917, glen=425, tlen=570, kl=-0.000985, cri=0.0173, vals=0.918][A
Train epoch [1/1]:  69%|██████▉   | 88/128 [09:23<04:07,  6.18s/it, pg=0.351, rm=0.769, ret=0.77, glen=297, tlen=423, kl=-0.00058, cri=0.0345, vals=0.746]   [A
Train epoch [1/1]:  70%|██████▉   | 89/128 [09:23<03:58,  6.11s/it, pg=0.351, rm=0.769, ret=0.77, glen=297, tlen=423, kl=-0.00058, cri=0.0345, vals=0.746][A
Train epoch [1/1]:  70%|██████▉   | 89/128 [09:29<03:58,  6.11s/it, pg=0.347, rm=0.624, ret=0.626, glen=241, tlen=464, kl=-0.000849, cri=0.0267, vals=0.905][A
Train epoch [1/1]:  70%|███████   | 90/128 [09:29<03:54,  6.16s/it, pg=0.347, rm=0.624, ret=0.626, glen=241, tlen=464, kl=-0.000849, cri=0.0267, vals=0.905][A
Train epoch [1/1]:  70%|███████   | 90/128 [09:36<03:54,  6.16s/it, pg=0.147, rm=1.08, ret=1.08, glen=339, tlen=557, kl=-0.000549, cri=0.0173, vals=1.09]   [A
Train epoch [1/1]:  71%|███████   | 91/128 [09:36<03:49,  6.21s/it, pg=0.147, rm=1.08, ret=1.08, glen=339, tlen=557, kl=-0.000549, cri=0.0173, vals=1.09][A
Train epoch [1/1]:  71%|███████   | 91/128 [09:42<03:49,  6.21s/it, pg=-0.172, rm=1.3, ret=1.3, glen=345, tlen=496, kl=-5.81e-7, cri=0.0223, vals=1.04]  [A
Train epoch [1/1]:  72%|███████▏  | 92/128 [09:42<03:42,  6.18s/it, pg=-0.172, rm=1.3, ret=1.3, glen=345, tlen=496, kl=-5.81e-7, cri=0.0223, vals=1.04][A
Train epoch [1/1]:  72%|███████▏  | 92/128 [09:48<03:42,  6.18s/it, pg=-0.07, rm=1.17, ret=1.17, glen=363, tlen=542, kl=0.000989, cri=0.0154, vals=1.02][A
Train epoch [1/1]:  73%|███████▎  | 93/128 [09:48<03:34,  6.13s/it, pg=-0.07, rm=1.17, ret=1.17, glen=363, tlen=542, kl=0.000989, cri=0.0154, vals=1.02][A
Train epoch [1/1]:  73%|███████▎  | 93/128 [09:54<03:34,  6.13s/it, pg=0.19, rm=1.02, ret=1.02, glen=336, tlen=554, kl=-0.0003, cri=0.0217, vals=0.965] [A
Train epoch [1/1]:  73%|███████▎  | 94/128 [09:54<03:28,  6.13s/it, pg=0.19, rm=1.02, ret=1.02, glen=336, tlen=554, kl=-0.0003, cri=0.0217, vals=0.965][A
Train epoch [1/1]:  73%|███████▎  | 94/128 [10:00<03:28,  6.13s/it, pg=0.115, rm=0.934, ret=0.935, glen=218, tlen=420, kl=-0.000204, cri=0.0643, vals=0.716][A
Train epoch [1/1]:  74%|███████▍  | 95/128 [10:00<03:20,  6.06s/it, pg=0.115, rm=0.934, ret=0.935, glen=218, tlen=420, kl=-0.000204, cri=0.0643, vals=0.716][A
Train epoch [1/1]:  74%|███████▍  | 95/128 [10:13<03:20,  6.06s/it, pg=0.766, rm=0.735, ret=0.735, glen=277, tlen=488, kl=-8.23e-6, cri=0.0458, vals=1.21]  [A
Train epoch [1/1]:  75%|███████▌  | 96/128 [10:13<04:23,  8.25s/it, pg=0.766, rm=0.735, ret=0.735, glen=277, tlen=488, kl=-8.23e-6, cri=0.0458, vals=1.21][A
Train epoch [1/1]:  75%|███████▌  | 96/128 [10:18<04:23,  8.25s/it, pg=1.32, rm=0.398, ret=0.397, glen=309, tlen=474, kl=0.000182, cri=0.157, vals=0.815] [A
Train epoch [1/1]:  76%|███████▌  | 97/128 [10:18<03:43,  7.22s/it, pg=1.32, rm=0.398, ret=0.397, glen=309, tlen=474, kl=0.000182, cri=0.157, vals=0.815][A
Train epoch [1/1]:  76%|███████▌  | 97/128 [10:24<03:43,  7.22s/it, pg=0.412, rm=0.645, ret=0.645, glen=294, tlen=431, kl=1.76e-5, cri=0.0412, vals=0.837][A
Train epoch [1/1]:  77%|███████▋  | 98/128 [10:24<03:24,  6.83s/it, pg=0.412, rm=0.645, ret=0.645, glen=294, tlen=431, kl=1.76e-5, cri=0.0412, vals=0.837][A
Train epoch [1/1]:  77%|███████▋  | 98/128 [10:30<03:24,  6.83s/it, pg=0.0858, rm=1.08, ret=1.08, glen=296, tlen=449, kl=0.000308, cri=0.0383, vals=1.08] [A
Train epoch [1/1]:  77%|███████▋  | 99/128 [10:30<03:13,  6.66s/it, pg=0.0858, rm=1.08, ret=1.08, glen=296, tlen=449, kl=0.000308, cri=0.0383, vals=1.08][A
Train epoch [1/1]:  77%|███████▋  | 99/128 [10:36<03:13,  6.66s/it, pg=0.216, rm=0.648, ret=0.65, glen=363, tlen=565, kl=-0.000681, cri=0.0186, vals=0.966][A
Train epoch [1/1]:  78%|███████▊  | 100/128 [10:36<03:02,  6.53s/it, pg=0.216, rm=0.648, ret=0.65, glen=363, tlen=565, kl=-0.000681, cri=0.0186, vals=0.966][A
Train epoch [1/1]:  78%|███████▊  | 100/128 [10:42<03:02,  6.53s/it, pg=0.0195, rm=0.687, ret=0.686, glen=190, tlen=331, kl=0.000306, cri=0.0472, vals=0.706][A
Train epoch [1/1]:  79%|███████▉  | 101/128 [10:42<02:51,  6.34s/it, pg=0.0195, rm=0.687, ret=0.686, glen=190, tlen=331, kl=0.000306, cri=0.0472, vals=0.706][A
Train epoch [1/1]:  79%|███████▉  | 101/128 [10:48<02:51,  6.34s/it, pg=-0.0133, rm=1.08, ret=1.07, glen=346, tlen=412, kl=0.00157, cri=0.0219, vals=0.982]  [A
Train epoch [1/1]:  80%|███████▉  | 102/128 [10:48<02:40,  6.19s/it, pg=-0.0133, rm=1.08, ret=1.07, glen=346, tlen=412, kl=0.00157, cri=0.0219, vals=0.982][A
Train epoch [1/1]:  80%|███████▉  | 102/128 [10:54<02:40,  6.19s/it, pg=0.0388, rm=0.965, ret=0.96, glen=302, tlen=468, kl=0.00151, cri=0.0208, vals=0.889][A
Train epoch [1/1]:  80%|████████  | 103/128 [10:54<02:35,  6.20s/it, pg=0.0388, rm=0.965, ret=0.96, glen=302, tlen=468, kl=0.00151, cri=0.0208, vals=0.889][A
Train epoch [1/1]:  80%|████████  | 103/128 [11:00<02:35,  6.20s/it, pg=0.724, rm=0.748, ret=0.746, glen=255, tlen=413, kl=0.000935, cri=0.0641, vals=0.78][A
Train epoch [1/1]:  81%|████████▏ | 104/128 [11:00<02:25,  6.08s/it, pg=0.724, rm=0.748, ret=0.746, glen=255, tlen=413, kl=0.000935, cri=0.0641, vals=0.78][A
Train epoch [1/1]:  81%|████████▏ | 104/128 [11:06<02:25,  6.08s/it, pg=0.0292, rm=0.573, ret=0.573, glen=285, tlen=414, kl=-0.000224, cri=0.0287, vals=0.871][A
Train epoch [1/1]:  82%|████████▏ | 105/128 [11:06<02:19,  6.07s/it, pg=0.0292, rm=0.573, ret=0.573, glen=285, tlen=414, kl=-0.000224, cri=0.0287, vals=0.871][A
Train epoch [1/1]:  82%|████████▏ | 105/128 [11:12<02:19,  6.07s/it, pg=-0.0212, rm=0.8, ret=0.801, glen=276, tlen=441, kl=-8.56e-5, cri=0.0577, vals=1.09]   [A
Train epoch [1/1]:  83%|████████▎ | 106/128 [11:12<02:13,  6.06s/it, pg=-0.0212, rm=0.8, ret=0.801, glen=276, tlen=441, kl=-8.56e-5, cri=0.0577, vals=1.09][A
Train epoch [1/1]:  83%|████████▎ | 106/128 [11:18<02:13,  6.06s/it, pg=0.309, rm=0.769, ret=0.77, glen=302, tlen=452, kl=-0.000342, cri=0.0241, vals=0.851][A
Train epoch [1/1]:  84%|████████▎ | 107/128 [11:18<02:07,  6.07s/it, pg=0.309, rm=0.769, ret=0.77, glen=302, tlen=452, kl=-0.000342, cri=0.0241, vals=0.851][A
Train epoch [1/1]:  84%|████████▎ | 107/128 [11:24<02:07,  6.07s/it, pg=-0.338, rm=1.11, ret=1.11, glen=246, tlen=421, kl=-0.000717, cri=0.0383, vals=0.991][A
Train epoch [1/1]:  84%|████████▍ | 108/128 [11:24<02:00,  6.04s/it, pg=-0.338, rm=1.11, ret=1.11, glen=246, tlen=421, kl=-0.000717, cri=0.0383, vals=0.991][A
Train epoch [1/1]:  84%|████████▍ | 108/128 [11:30<02:00,  6.04s/it, pg=-0.0755, rm=1.07, ret=1.07, glen=399, tlen=512, kl=0.00124, cri=0.0105, vals=0.972] [A
Train epoch [1/1]:  85%|████████▌ | 109/128 [11:30<01:54,  6.02s/it, pg=-0.0755, rm=1.07, ret=1.07, glen=399, tlen=512, kl=0.00124, cri=0.0105, vals=0.972][A
Train epoch [1/1]:  85%|████████▌ | 109/128 [11:36<01:54,  6.02s/it, pg=0.81, rm=0.406, ret=0.408, glen=234, tlen=321, kl=-0.000794, cri=0.0942, vals=0.889][A
Train epoch [1/1]:  86%|████████▌ | 110/128 [11:36<01:48,  6.00s/it, pg=0.81, rm=0.406, ret=0.408, glen=234, tlen=321, kl=-0.000794, cri=0.0942, vals=0.889][A
Train epoch [1/1]:  86%|████████▌ | 110/128 [11:42<01:48,  6.00s/it, pg=0.403, rm=0.776, ret=0.775, glen=239, tlen=482, kl=0.000735, cri=0.0246, vals=0.914][A
Train epoch [1/1]:  87%|████████▋ | 111/128 [11:42<01:42,  6.06s/it, pg=0.403, rm=0.776, ret=0.775, glen=239, tlen=482, kl=0.000735, cri=0.0246, vals=0.914][A
Train epoch [1/1]:  87%|████████▋ | 111/128 [11:56<01:42,  6.06s/it, pg=0.287, rm=0.188, ret=0.187, glen=314, tlen=491, kl=0.000164, cri=0.0317, vals=0.374][A
Train epoch [1/1]:  88%|████████▊ | 112/128 [11:56<02:12,  8.30s/it, pg=0.287, rm=0.188, ret=0.187, glen=314, tlen=491, kl=0.000164, cri=0.0317, vals=0.374][A
Train epoch [1/1]:  88%|████████▊ | 112/128 [12:01<02:12,  8.30s/it, pg=0.0224, rm=1.25, ret=1.24, glen=282, tlen=375, kl=0.000138, cri=0.0167, vals=1.24]  [A
Train epoch [1/1]:  88%|████████▊ | 113/128 [12:01<01:48,  7.21s/it, pg=0.0224, rm=1.25, ret=1.24, glen=282, tlen=375, kl=0.000138, cri=0.0167, vals=1.24][A
Train epoch [1/1]:  88%|████████▊ | 113/128 [12:07<01:48,  7.21s/it, pg=1.34, rm=0.155, ret=0.155, glen=137, tlen=382, kl=0.000138, cri=0.117, vals=0.436][A
Train epoch [1/1]:  89%|████████▉ | 114/128 [12:07<01:36,  6.88s/it, pg=1.34, rm=0.155, ret=0.155, glen=137, tlen=382, kl=0.000138, cri=0.117, vals=0.436][A
Train epoch [1/1]:  89%|████████▉ | 114/128 [12:13<01:36,  6.88s/it, pg=0.224, rm=0.884, ret=0.881, glen=272, tlen=427, kl=0.00111, cri=0.0318, vals=1.04][A
Train epoch [1/1]:  90%|████████▉ | 115/128 [12:13<01:26,  6.66s/it, pg=0.224, rm=0.884, ret=0.881, glen=272, tlen=427, kl=0.00111, cri=0.0318, vals=1.04][A
Train epoch [1/1]:  90%|████████▉ | 115/128 [12:19<01:26,  6.66s/it, pg=-0.272, rm=0.767, ret=0.769, glen=254, tlen=401, kl=-0.000557, cri=0.0277, vals=0.693][A
Train epoch [1/1]:  91%|█████████ | 116/128 [12:19<01:17,  6.43s/it, pg=-0.272, rm=0.767, ret=0.769, glen=254, tlen=401, kl=-0.000557, cri=0.0277, vals=0.693][A
Train epoch [1/1]:  91%|█████████ | 116/128 [12:25<01:17,  6.43s/it, pg=-0.164, rm=1.12, ret=1.12, glen=292, tlen=484, kl=0.000199, cri=0.0168, vals=0.978]   [A
Train epoch [1/1]:  91%|█████████▏| 117/128 [12:25<01:09,  6.34s/it, pg=-0.164, rm=1.12, ret=1.12, glen=292, tlen=484, kl=0.000199, cri=0.0168, vals=0.978][A
Train epoch [1/1]:  91%|█████████▏| 117/128 [12:31<01:09,  6.34s/it, pg=0.12, rm=1.05, ret=1.05, glen=343, tlen=425, kl=-0.000399, cri=0.0144, vals=0.984] [A
Train epoch [1/1]:  92%|█████████▏| 118/128 [12:31<01:02,  6.20s/it, pg=0.12, rm=1.05, ret=1.05, glen=343, tlen=425, kl=-0.000399, cri=0.0144, vals=0.984][A
Train epoch [1/1]:  92%|█████████▏| 118/128 [12:37<01:02,  6.20s/it, pg=0.356, rm=0.623, ret=0.622, glen=282, tlen=402, kl=0.000311, cri=0.0192, vals=1.09][A
Train epoch [1/1]:  93%|█████████▎| 119/128 [12:37<00:54,  6.10s/it, pg=0.356, rm=0.623, ret=0.622, glen=282, tlen=402, kl=0.000311, cri=0.0192, vals=1.09][A
Train epoch [1/1]:  93%|█████████▎| 119/128 [12:42<00:54,  6.10s/it, pg=0.443, rm=0.794, ret=0.795, glen=205, tlen=350, kl=-0.000437, cri=0.0661, vals=0.939][A
Train epoch [1/1]:  94%|█████████▍| 120/128 [12:42<00:48,  6.05s/it, pg=0.443, rm=0.794, ret=0.795, glen=205, tlen=350, kl=-0.000437, cri=0.0661, vals=0.939][A
Train epoch [1/1]:  94%|█████████▍| 120/128 [12:49<00:48,  6.05s/it, pg=-0.0953, rm=1.24, ret=1.24, glen=317, tlen=421, kl=-0.000347, cri=0.0247, vals=1.15] [A
Train epoch [1/1]:  95%|█████████▍| 121/128 [12:49<00:42,  6.08s/it, pg=-0.0953, rm=1.24, ret=1.24, glen=317, tlen=421, kl=-0.000347, cri=0.0247, vals=1.15][A
Train epoch [1/1]:  95%|█████████▍| 121/128 [12:55<00:42,  6.08s/it, pg=0.0726, rm=0.623, ret=0.625, glen=336, tlen=480, kl=-0.000506, cri=0.0278, vals=0.963][A
Train epoch [1/1]:  95%|█████████▌| 122/128 [12:55<00:36,  6.08s/it, pg=0.0726, rm=0.623, ret=0.625, glen=336, tlen=480, kl=-0.000506, cri=0.0278, vals=0.963][A
Train epoch [1/1]:  95%|█████████▌| 122/128 [13:01<00:36,  6.08s/it, pg=-0.226, rm=0.975, ret=0.974, glen=238, tlen=418, kl=0.000177, cri=0.0313, vals=1.08]  [A
Train epoch [1/1]:  96%|█████████▌| 123/128 [13:01<00:30,  6.09s/it, pg=-0.226, rm=0.975, ret=0.974, glen=238, tlen=418, kl=0.000177, cri=0.0313, vals=1.08][A
Train epoch [1/1]:  96%|█████████▌| 123/128 [13:07<00:30,  6.09s/it, pg=0.342, rm=1.01, ret=1.01, glen=383, tlen=456, kl=0.000214, cri=0.0255, vals=0.986]  [A
Train epoch [1/1]:  97%|█████████▋| 124/128 [13:07<00:24,  6.04s/it, pg=0.342, rm=1.01, ret=1.01, glen=383, tlen=456, kl=0.000214, cri=0.0255, vals=0.986][A
Train epoch [1/1]:  97%|█████████▋| 124/128 [13:13<00:24,  6.04s/it, pg=-0.0274, rm=0.933, ret=0.93, glen=351, tlen=588, kl=0.000935, cri=0.0123, vals=0.914][A
Train epoch [1/1]:  98%|█████████▊| 125/128 [13:13<00:18,  6.19s/it, pg=-0.0274, rm=0.933, ret=0.93, glen=351, tlen=588, kl=0.000935, cri=0.0123, vals=0.914][A
Train epoch [1/1]:  98%|█████████▊| 125/128 [13:19<00:18,  6.19s/it, pg=-0.0383, rm=1.19, ret=1.19, glen=374, tlen=511, kl=-0.000468, cri=0.013, vals=1.27]  [A
Train epoch [1/1]:  98%|█████████▊| 126/128 [13:19<00:12,  6.15s/it, pg=-0.0383, rm=1.19, ret=1.19, glen=374, tlen=511, kl=-0.000468, cri=0.013, vals=1.27][A
Train epoch [1/1]:  98%|█████████▊| 126/128 [13:25<00:12,  6.15s/it, pg=0.23, rm=0.876, ret=0.875, glen=325, tlen=504, kl=0.000374, cri=0.0188, vals=0.693][A
Train epoch [1/1]:  99%|█████████▉| 127/128 [13:25<00:06,  6.08s/it, pg=0.23, rm=0.876, ret=0.875, glen=325, tlen=504, kl=0.000374, cri=0.0188, vals=0.693][A
Train epoch [1/1]:  99%|█████████▉| 127/128 [13:38<00:06,  6.08s/it, pg=0.409, rm=0.901, ret=0.902, glen=372, tlen=420, kl=-0.00013, cri=0.0807, vals=0.986][A
Train epoch [1/1]: 100%|██████████| 128/128 [13:38<00:00,  8.15s/it, pg=0.409, rm=0.901, ret=0.902, glen=372, tlen=420, kl=-0.00013, cri=0.0807, vals=0.986][ATrain epoch [1/1]: 100%|██████████| 128/128 [13:38<00:00,  6.40s/it, pg=0.409, rm=0.901, ret=0.902, glen=372, tlen=420, kl=-0.00013, cri=0.0807, vals=0.986]
Episode [1/1]:   3%|▎         | 127/3742 [1:46:32<38:37:38, 38.47s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   3%|▎         | 128/3742 [1:46:32<305:03:12, 303.87s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   3%|▎         | 129/3742 [1:47:09<224:37:49, 223.82s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   3%|▎         | 130/3742 [1:47:46<168:21:42, 167.80s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▎         | 131/3742 [1:48:24<129:20:48, 128.95s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▎         | 132/3742 [1:49:03<102:11:36, 101.91s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▎         | 133/3742 [1:49:40<82:37:16, 82.42s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]  Episode [1/1]:   4%|▎         | 134/3742 [1:50:20<69:46:58, 69.63s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▎         | 135/3742 [1:50:56<59:38:56, 59.53s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▎         | 136/3742 [1:51:34<53:21:20, 53.27s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▎         | 137/3742 [1:52:12<48:33:28, 48.49s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▎         | 138/3742 [1:52:40<42:23:45, 42.35s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▎         | 139/3742 [1:53:19<41:34:33, 41.54s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▎         | 140/3742 [1:53:59<41:01:50, 41.01s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 141/3742 [1:54:38<40:24:32, 40.40s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 142/3742 [1:54:50<31:50:23, 31.84s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 143/3742 [1:55:28<33:35:28, 33.60s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 144/3742 [1:56:06<34:54:49, 34.93s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 145/3742 [1:56:15<27:05:40, 27.12s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 146/3742 [1:56:52<30:06:17, 30.14s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 147/3742 [1:57:30<32:34:53, 32.63s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 148/3742 [1:57:54<29:53:06, 29.94s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 149/3742 [1:58:33<32:37:23, 32.69s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 150/3742 [1:59:13<34:40:25, 34.75s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 151/3742 [1:59:50<35:26:55, 35.54s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 152/3742 [2:00:28<36:14:01, 36.33s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 153/3742 [2:01:07<37:03:17, 37.17s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 154/3742 [2:01:47<37:44:29, 37.87s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 155/3742 [2:02:26<38:14:16, 38.38s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 156/3742 [2:03:03<37:38:55, 37.80s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 157/3742 [2:03:40<37:25:09, 37.58s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 158/3742 [2:04:18<37:36:25, 37.77s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 159/3742 [2:04:55<37:22:59, 37.56s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 160/3742 [2:05:33<37:22:44, 37.57s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 161/3742 [2:06:12<37:57:53, 38.17s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 162/3742 [2:06:51<37:59:47, 38.21s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 163/3742 [2:07:28<37:45:41, 37.98s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 164/3742 [2:08:08<38:12:48, 38.45s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 165/3742 [2:08:47<38:32:15, 38.79s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 166/3742 [2:09:27<38:48:55, 39.08s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 167/3742 [2:09:59<36:37:24, 36.88s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   4%|▍         | 168/3742 [2:10:36<36:43:52, 37.00s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 169/3742 [2:11:15<37:12:11, 37.48s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 170/3742 [2:11:53<37:27:35, 37.75s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 171/3742 [2:12:30<37:16:13, 37.57s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 172/3742 [2:13:08<37:21:00, 37.66s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 173/3742 [2:13:46<37:17:33, 37.62s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 174/3742 [2:14:23<37:05:34, 37.43s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 175/3742 [2:15:00<37:06:21, 37.45s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 176/3742 [2:15:38<37:17:06, 37.64s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 177/3742 [2:16:18<37:50:56, 38.22s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 178/3742 [2:16:57<38:06:48, 38.50s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 179/3742 [2:17:04<28:53:41, 29.19s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]Episode [1/1]:   5%|▍         | 180/3742 [2:17:36<29:29:22, 29.80s/it, policy_loss=0.212, kl=-4.94e-5, reward=0.812, return=0.813, response_length=299, total_length=440, critic_loss=0.0329, values=0.92]